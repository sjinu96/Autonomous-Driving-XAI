{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfc729e",
   "metadata": {},
   "source": [
    "# Adversarial Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f90b02",
   "metadata": {},
   "source": [
    "### 0610\n",
    "offset 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e7f3e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc44f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import torch\n",
    "\n",
    "### local imports\n",
    "import model\n",
    "\n",
    "\n",
    "### Environment imports\n",
    "import click # Class object() 대신 argument 조절 library 사용\n",
    "import math\n",
    "import os\n",
    "import shutil # 고수준 파일 연산\n",
    "import torch\n",
    "import torch.utils.data.dataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3581f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration settings\n",
    "# Resnet의 기본 학습 단계이다. \n",
    "# Training rate / size parameters\n",
    "TRAIN_BATCHSIZE = 128  # GPU 개수에 따라 배수로 조절해도 됨.\n",
    "TRAIN_LR = 0.1  # GPU 개수에 따라 배수로 조절해도 됨. --> ?\n",
    "TRAIN_MOMENTUM = 0.9 #  --> (SGD optimizer 사용)\n",
    "TRAIN_WEIGHT_DECAY = 1e-4 # --> (SGD optimizer 사용)\n",
    "TRAIN_EPOCHS = [170, 195, 200]  # Divide lr by 10 at each; finish after last. --> 해당 epoch이 될 때마다 학습률을 0.1배\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01458e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial training settings\n",
    "# 적대적 훈련은 노이즈에 대한 L2 loss를 최소화함\n",
    "# 동시에 번갈아가면서 class에서 멀어지도록 gradient ascent를 진행..\n",
    "TRAIN_ADV_EPS = 0.01 # 적대적 설명을 사용한다.\n",
    "TRAIN_ADV_L2MIN_EPS = 0.1 # 적대적 설명을 사용할 때 L2 MIN도 사용가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326ef37",
   "metadata": {},
   "source": [
    "#### 추가적 이해 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적대적 설명에 대한 예시\n",
    "\"\"\"Trains a network and saves the result at PATH.\n",
    "\n",
    "    Options:\n",
    "\n",
    "        --adversarial-training: Train the network using adversarial examples.\n",
    "                By default, the adversarial examples are generated using a\n",
    "                standard \"L_2\" loss function and an epsilon of 0.01.\n",
    "\n",
    "        --l2-min: Only valid with --adversarial-training.  If specified, use\n",
    "                the \"L_{2,min}\"  method of generating adversarial examples,\n",
    "                with an epsilon of 0.1. # 적대적 학습의 L2 min. \n",
    "\n",
    "        --robust-additions: Train with the best settings of the other\n",
    "                modifications in the paper, including: defense via Lipschitz\n",
    "                Continuity with \"L_{adv,z=2}\", \"\\zeta = 0.2\" using\n",
    "                \"L_{adv,tandem}\", the Half-Huber ReLU, no output zeroing,\n",
    "                an adaptive \"\\psi\" with \"L_{target}=1.5\", \"k_{\\psi,0}=220\",\n",
    "                \"k_{\\psi}=\\ln 0.02\", \"\\epsilon_{better}=1\",\n",
    "                \"\\epsilon_{worse}=0.01\", and half-half adversarial training when\n",
    "                also using adversarial training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeae06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial robustness parameters\n",
    "ROBUST_Z = 2\n",
    "ROBUST_ZETA = 0.2  # 항상 쌍으로 이루어진다(tandem).\n",
    "ROBUST_ADAPT_L_TARGET = 1.5\n",
    "ROBUST_ADAPT_PSI_0 = 220\n",
    "ROBUST_ADAPT_PSI = 0.02\n",
    "ROBUST_ADAPT_EPS_POS = 1\n",
    "ROBUST_ADAPT_EPS_NEG = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d7ed6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60d6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset as [mean, std] of data input. --> ( Resnet 메인 모델의 forward, 정규화)\n",
    "MODEL_INPUT_OFFSET = [[0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cddd58",
   "metadata": {},
   "source": [
    "### 네트워크 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafae4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_preprocess(ft_out):\n",
    "    return torch.nn.Conv2d(3, ft_out, kernel_size=3, padding=1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac80bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ARCH=[\n",
    "    32, # input size, 정사각형 가정\n",
    "    cifar10_preprocess, #Resnet의 initial layer.\n",
    "    [(44 - 2)//6 for _ in range(3)],# [7,7,7],  Block Length --> ( ... ) \n",
    "    [16, 32, 64], # feature의 개수들.\n",
    "    10,  # class 개수 (cifar-10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-resource computer를 위한 실험 (우선 배제)\n",
    "# ONE_BATCH_ONLY = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde9449",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef69fa",
   "metadata": {},
   "source": [
    "### automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6460aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_path='../BDD100K_MOT2020_image/bdd100k/images/track/new_train' 0609\n",
    "\n",
    "\n",
    "root='../BDD100K_MOT2020_image/bdd100k/images/track/train_av'\n",
    "root_test='../BDD100K_MOT2020_image/bdd100k/images/track/test_av'\n",
    "\n",
    "label_save_path='../BDD100K_MOT2020_label/bdd100k/labels/box_track_20/new_train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98541b",
   "metadata": {},
   "source": [
    "#### 기존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0825a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset():\n",
    "    \n",
    "    \n",
    "    \"\"\"Returns (ds_train, ds_test) with augmentations on training set.\n",
    "    \"\"\"\n",
    "    T = torchvision.transforms\n",
    "#     aug_pad = 4\n",
    "#     aug_dim = 32 + aug_pad * 2\n",
    "    transform_train = T.Compose([T.ToTensor(), ])\n",
    "        \n",
    "    ds_train = torchvision.datasets.ImageFolder(root, \n",
    "            transform=T.ToTensor())\n",
    "    ds_test = torchvision.datasets.ImageFolder(root_test, \n",
    "            transform=T.ToTensor())\n",
    "    \n",
    "    return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test=get"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e121ac",
   "metadata": {},
   "source": [
    "#### 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "8f5accb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bicycle',\n",
       " 'bus',\n",
       " 'car',\n",
       " 'motorcycle',\n",
       " 'other person',\n",
       " 'other vehicle',\n",
       " 'pedestrian',\n",
       " 'rider',\n",
       " 'trailer',\n",
       " 'train',\n",
       " 'truck']"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf427b",
   "metadata": {},
   "source": [
    "증강사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "e2e18945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset2():\n",
    "    \"\"\"Returns (ds_train, ds_test) with augmentations on training set.\n",
    "    \"\"\"\n",
    "    T = torchvision.transforms\n",
    "    aug_pad = 4\n",
    "    aug_dim = 32 + aug_pad * 2\n",
    "    ds_train = torchvision.datasets.ImageFolder(root, \n",
    "            transform=T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda tensor:\n",
    "                    torch.nn.functional.pad(\n",
    "                        tensor.view(1, 3, 32, 32),\n",
    "                        (aug_pad,)*4,\n",
    "                        'replicate').view(3, aug_dim, aug_dim)),\n",
    "                T.ToPILImage(),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomCrop((32, 32)),\n",
    "                T.ToTensor(),\n",
    "            ]))\n",
    "    ds_test = torchvision.datasets.ImageFolder(root_test,\n",
    "            transform=torchvision.transforms.ToTensor())\n",
    "    return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b68c6d",
   "metadata": {},
   "source": [
    "### Train, Test Image Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "6048e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trailer 305\n",
      "motorcycle 983\n",
      "bicycle 3505\n",
      "car 57212\n",
      "bus 7549\n",
      "other vehicle 2699\n",
      "pedestrian 44011\n",
      "other person 422\n",
      "truck 15033\n",
      "train 3\n",
      "rider 1919\n"
     ]
    }
   ],
   "source": [
    "labels=os.listdir(root)\n",
    "for label in labels:\n",
    "    print(label, len(os.listdir(os.path.join(root,label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcaf32f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trailer 990\n",
      "motorcycle 8114\n",
      "bicycle 19637\n",
      "car 102971\n",
      "bus 40409\n",
      "other vehicle 13644\n",
      "pedestrian 104779\n",
      "other person 1974\n",
      "truck 99964\n",
      "train 1064\n",
      "rider 13336\n"
     ]
    }
   ],
   "source": [
    "labels=os.listdir(root)\n",
    "for label in labels:\n",
    "    print(label, len(os.listdir(os.path.join(root,label))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d9de1",
   "metadata": {},
   "source": [
    "> root_test는 추후에 밸런스 맞춰서 할당."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f1416cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "176\n",
      "1086\n",
      "14257\n",
      "981\n",
      "469\n",
      "9427\n",
      "130\n",
      "4520\n",
      "1\n",
      "397\n"
     ]
    }
   ],
   "source": [
    "labels=os.listdir(root_test)\n",
    "for label in labels:\n",
    "    print(len(os.listdir(os.path.join(root_test,label))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c28f95",
   "metadata": {},
   "source": [
    "22:06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a6ab8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test=_get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd42b613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 31495\n",
       "    Root location: ../BDD100K_MOT2020_image/bdd100k/images/track/test_av\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "bd30cdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bicycle': 0,\n",
       " 'bus': 1,\n",
       " 'car': 2,\n",
       " 'motorcycle': 3,\n",
       " 'other person': 4,\n",
       " 'other vehicle': 5,\n",
       " 'pedestrian': 6,\n",
       " 'rider': 7,\n",
       " 'trailer': 8,\n",
       " 'train': 9,\n",
       " 'truck': 10}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "13a58359",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "1536590e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(ds_train[i][1])\n",
    "img=ds_train[i][0].numpy()\n",
    "img=np.swapaxes(img,0,1)\n",
    "img=np.swapaxes(img,1,2)\n",
    "plt.imshow(img)\n",
    "i+=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e1d43",
   "metadata": {},
   "source": [
    "### 셔플 잘 되나."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "a8e920c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ds_train, batch_size=128,\n",
    "            shuffle=True, num_workers=8, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "94352161",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "3e3a255e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fb7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e6875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df609783",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "56cb117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebe9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "b396eaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bicycle': 0,\n",
       " 'bus': 1,\n",
       " 'car': 2,\n",
       " 'motorcycle': 3,\n",
       " 'other person': 4,\n",
       " 'other vehicle': 5,\n",
       " 'pedestrian': 6,\n",
       " 'rider': 7,\n",
       " 'trailer': 8,\n",
       " 'train': 9,\n",
       " 'truck': 10}"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "8751d33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(batch[1][i])\n",
    "img=batch[0][i].numpy()\n",
    "img=np.swapaxes(img,0,1)\n",
    "img=np.swapaxes(img,1,2)\n",
    "plt.imshow(img)\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbf473",
   "metadata": {},
   "source": [
    "셔플은 잘 된듯."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed100fe3",
   "metadata": {},
   "source": [
    "#### 데이터증강 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "c4b1d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test=_get_dataset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "92003d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 133641\n",
       "    Root location: ../BDD100K_MOT2020_image/bdd100k/images/track/train_av\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Lambda()\n",
       "               ToPILImage()\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomCrop(size=(32, 32), padding=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "87a2d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "9cfe5079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(ds_train[i][1])\n",
    "img=ds_train[i][0].numpy()\n",
    "img=np.swapaxes(img,0,1)\n",
    "img=np.swapaxes(img,1,2)\n",
    "plt.imshow(img)\n",
    "i+=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741dfec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f2d6cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_cifar():\n",
    "    \"\"\"Returns (ds_train, ds_test) with augmentations on training set.\n",
    "    \"\"\"\n",
    "    T = torchvision.transforms\n",
    "    aug_pad = 4\n",
    "    aug_dim = 32 + aug_pad * 2\n",
    "    ds_train = torchvision.datasets.CIFAR10('CIFAR10_PATH', train=True,\n",
    "            transform=T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda tensor:\n",
    "                    torch.nn.functional.pad(\n",
    "                        tensor.view(1, 3, 32, 32),\n",
    "                        (aug_pad,)*4,\n",
    "                        'replicate').view(3, aug_dim, aug_dim)),\n",
    "                T.ToPILImage(),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomCrop((32, 32)),\n",
    "                T.ToTensor(),\n",
    "            ]))\n",
    "    ds_test = torchvision.datasets.CIFAR10('CIFAR10_PATH', train=False,\n",
    "            transform=torchvision.transforms.ToTensor())\n",
    "    return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cba92ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bdb116e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "63035923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "16662f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "5859ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train[i][1])\n",
    "img=train[i][0].numpy()\n",
    "img=np.swapaxes(img,0,1)\n",
    "img=np.swapaxes(img,1,2)\n",
    "plt.imshow(img)\n",
    "i+=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fac7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=get_dataset_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915a56f",
   "metadata": {},
   "source": [
    "### cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c37a02d",
   "metadata": {},
   "source": [
    "#### 우선 환경변수 부분은 건들지 않도록 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # CIFAR-10 찾기\n",
    "# # # CIFAR10_PATH에 관한 환경변수가 없다면 None을 반환한다.\n",
    "# # cifar10_path = os.environ.get('CIFAR10_PATH', '')\n",
    "\n",
    "# if not cifar10_path.strip():\n",
    "# #     raise ValueError('Must specify CIFAR10_PATH environment variable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245adc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar10_path='CIFAR10_PATH/'\n",
    "\n",
    "# # 경로설정\n",
    "# if not os.path.exists(cifar10_path):\n",
    "#     os.mkdir(cifar10_path)\n",
    "# # dataset 다운로드\n",
    "# if len(os.listdir(cifar10_path))==0:\n",
    "#     torchvision.datasets.CIFAR10(cifar10_path, download=True)\n",
    "\n",
    "# Labels\n",
    "# class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog',\n",
    "#         'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c282d",
   "metadata": {},
   "source": [
    "## 커맨드라인 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b47af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87368166",
   "metadata": {},
   "outputs": [],
   "source": [
    "@click.group()\n",
    "def main():\n",
    "    \"\"\"Resnet-44 network를 학습하고, 설명하는 함수\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "# 특정 path에 대해 Attack and BTR ARAs(Accuracy-Robustness Area)를 구하는 함수 \n",
    "@main.command()\n",
    "@click.argument('path') # argument와 option의 차이점 -->(option은 특성을 더 가진다) )\n",
    "@click.option('--n-images', default=1000, type=int) # --를 붙히는 이유 (option이라? 아마 약속)\n",
    "@click.option('--eps', default=20, type=float) # (... )에 대한 epsilon\n",
    "@click.option('--steps', default=450, type=int) # (... )에 대한 steps\n",
    "@click.option('--momentum', default=0.9, type=float)\n",
    "def calculate_ara(path, n_images, eps, steps, momentum):\n",
    "    \"\"\"Calculates the Attack and BTR Accuracy-Robustness Areas (ARAs) for\n",
    "    PATH.\n",
    "\n",
    "    Options:\n",
    "\n",
    "        --n-images: Number of images to use for the ARA calculation.  Defaults\n",
    "                to 1000.\n",
    "\n",
    "        --eps: Maximum perturbation to apply.  May be greater than 1; per\n",
    "                Algorithm 1, many of the steps taken will minimize the\n",
    "                perturbation magnitude instead of decreasing confidence in the\n",
    "                correct class.  Defaults to 450*.1*.5 = 20, which is about what\n",
    "                the paper used.\n",
    "\n",
    "        --steps: Number of steps to use in determining the lowest-magnitude\n",
    "                perturbation which results in a change of either accuracy or\n",
    "                BTR, based on the ARA being calculated.  Defaults to 450.\n",
    "\n",
    "        --momentum: If using fewer than 100 ``steps``, probably best to disable\n",
    "                momentum by setting it to zero.  Defaults to 0.9.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set up model\n",
    "    m = _model_load(path)\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        device = torch.device('cuda')\n",
    "        m = m.to(device)\n",
    "        m = torch.nn.DataParallel(m)\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    # Set up dataset based on first --n-images of random test-set permutation\n",
    "    ds_train, ds_test = _get_dataset()\n",
    "    state = torch.get_rng_state()\n",
    "    torch.manual_seed(1776)\n",
    "    idx_all = [int(i) for i in torch.randperm(len(ds_test))]\n",
    "    torch.set_rng_state(state)\n",
    "    ds_test = torch.utils.data.dataset.Subset(ds_test, idx_all[:n_images])\n",
    "\n",
    "    for ara_name, ara_type in [('Attack', True), ('BTR', 'btr')]:\n",
    "        batch_size = TRAIN_BATCHSIZE * max(1, torch.cuda.device_count())\n",
    "        test_loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size,\n",
    "                shuffle=False, num_workers=8, drop_last=False)\n",
    "\n",
    "        n = 0\n",
    "        diffs = []\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            n += images.size(0)\n",
    "\n",
    "            advs = _adv_images(m, images, labels, AdversarialOptions(\n",
    "                    steps=steps, eps=eps, eps_overshoot=1, use_l2min=ara_type,\n",
    "                    momentum=momentum))\n",
    "            mags = advs.sub_(images).pow_(2).mean((1, 2, 3)).sqrt_()\n",
    "            diffs.extend(mags.tolist())\n",
    "            print('.', end='', flush=True)\n",
    "        print('')\n",
    "        assert n == n_images\n",
    "        assert n == len(diffs)\n",
    "\n",
    "        # Convert from perturbations to percentages\n",
    "        naive_guess = 1. / m.module.training_options['arch'][-1]\n",
    "        diffs = torch.Tensor(diffs)\n",
    "        bins = 1000\n",
    "        diff_max = diffs.max().item()\n",
    "        hist = diffs.histc(bins, min=0, max=diff_max)\n",
    "        gap = diff_max / bins\n",
    "        ara = gap * (1 - hist.cumsum(0) / n_images).add_(-naive_guess).clamp_(min=0).sum()\n",
    "        print(f'{ara_name} ARA: {ara}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b9e3d",
   "metadata": {},
   "source": [
    "### 설명 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f80044f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a88ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "@main.command() # main은 여전히 위에서 정의한 main 함수이다. \n",
    "@click.argument('path')\n",
    "@click.option('--eps', default=0.1) # (...)에 대한 epsilon\n",
    "def explain(path, eps):\n",
    "    \"\"\"CIFAR-10 dataset의 첫 10개 test example에 대한 설명\n",
    "    \n",
    "    output/에 우선적으로 저장한다.\"\"\"\n",
    "    \n",
    "    output_dir='output'\n",
    "    \n",
    "    # output 폴더 내 하위 폴더, 파일을 모두 삭제 (굳이 os 말고 이걸 쓰는 이유는? 깔끔한 삭제일 듯)\n",
    "    try:\n",
    "        shutil.rmtree(output_dir)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    ds_train, ds_test=_get_dataset() # 아래에서 정의, tensor 형태로 저장\n",
    "    m= _model_load(path) # static, 아래에서 정의. \n",
    "    \n",
    "    # Determine cuda status\n",
    "    \n",
    "    device=torch.device('cpu') # 우선 cpu 설정\n",
    "    if torch.cuda.device_count() > 0: # gpu 이용 가능할 시 (cuda 잡혀있음)\n",
    "        device = torch.device('cuda') \n",
    "        m=m.to(device) # 모델을 device로 보내준다.\n",
    "        # small-bath explanations에 대해 Multi-GPU는 더 느리다.\n",
    "        #m=torch.nn.DataParallel(m)\n",
    "    \n",
    "    # Explanation 생성\n",
    "    \n",
    "    m.eval() # 추론 실행 전에 드롭 아웃 및 배치 정규화를 평가 모드로 설정해야 한다. \n",
    "    \n",
    "    for i in range(10): #1개\n",
    "        img, label = ds_test[i]\n",
    "      \"\"\"\n",
    "      이는 논문 외 적인 코드이므로 이후에 작성.\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \"\"\"\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f25882",
   "metadata": {},
   "source": [
    "### Adversarial 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdb2eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@main.command()\n",
    "@click.argument('path')\n",
    "@click.option('--adversarial-training/--no-adversarial-training', default=False)\n",
    "@click.option('--l2-min/--no-l2-min', default=False)\n",
    "@click.option('--robust-additions/--no-robust-additions', default=False)\n",
    "def train(path, **training_options):\n",
    "    \"\"\"Trains a network and saves the result at PATH.\n",
    "\n",
    "    Options:\n",
    "\n",
    "        --adversarial-training: Train the network using adversarial examples.\n",
    "                By default, the adversarial examples are generated using a\n",
    "                standard \"L_2\" loss function and an epsilon of 0.01.\n",
    "\n",
    "        --l2-min: Only valid with --adversarial-training.  If specified, use\n",
    "                the \"L_{2,min}\"  method of generating adversarial examples,\n",
    "                with an epsilon of 0.1. #적대적 학습의 L2 min. \n",
    "\n",
    "        --robust-additions: Train with the best settings of the other\n",
    "                modifications in the paper, including: defense via Lipschitz\n",
    "                Continuity with \"L_{adv,z=2}\", \"\\zeta = 0.2\" using\n",
    "                \"L_{adv,tandem}\", the Half-Huber ReLU, no output zeroing,\n",
    "                an adaptive \"\\psi\" with \"L_{target}=1.5\", \"k_{\\psi,0}=220\",\n",
    "                \"k_{\\psi}=\\ln 0.02\", \"\\epsilon_{better}=1\",\n",
    "                \"\\epsilon_{worse}=0.01\", and half-half adversarial training when\n",
    "                also using adversarial training.\n",
    "    \"\"\"\n",
    "    ds_train, ds_test = _get_dataset()\n",
    "    training_options['arch'] = MODEL_ARCH # 모델 구조\n",
    "    training_options['input_offset'] = MODEL_INPUT_OFFSET # data input의 평균, 분산 차이\n",
    "    m = model.Model(training_options) # model.MODEL : RESNET model.\n",
    "\n",
    "    # GPU의 개수를 정하고, 이에 따라 batch size와 learn rate을 설정하자.\n",
    "    g = max(torch.cuda.device_count(), 1)\n",
    "    batch_size = TRAIN_BATCHSIZE * g\n",
    "    opt = torch.optim.SGD(m.parameters(), lr=TRAIN_LR * g,\n",
    "            momentum=TRAIN_MOMENTUM, weight_decay=TRAIN_WEIGHT_DECAY)\n",
    "    # 이유는 솔직히 잘 모르겠다. ( ... ) \n",
    "    \n",
    "    # Move to GPU (필요하다면)\n",
    "    device = torch.device('cpu')\n",
    "    m_orig = m  # Preserve original reference for saving\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        device = torch.device('cuda')\n",
    "        m = m.to(device)\n",
    "        m = torch.nn.DataParallel(m)\n",
    "\n",
    "    # 훈련 Loop.\n",
    "    # ds_train은 dataset의 transform을 통해 Tensor로 바뀐 형태이다. \n",
    "    # train은 augmentation을 위한 패딩, RandomFlip, RandomCrop 등이 적용됐다.\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size,\n",
    "            shuffle=True, num_workers=8, drop_last=False)\n",
    "    test_loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size,\n",
    "            shuffle=False, num_workers=8, drop_last=False)\n",
    "    # num_workers = ( ... )\n",
    "    for epoch in range(max(TRAIN_EPOCHS)):\n",
    "        print(f'== {epoch} ==')\n",
    "        # Update learning rate\n",
    "        lr = TRAIN_LR * g # gpu개수\n",
    "        \n",
    "        for e in TRAIN_EPOCHS: # epoch이 진행될수록 학습률 낮추기. \n",
    "            if epoch >= e:\n",
    "                lr *= 0.1\n",
    "        for pg in opt.param_groups: # optimizer 학습률 반영\n",
    "            pg['lr'] = lr\n",
    "\n",
    "        #  training on batch\n",
    "        m.train() # m.eval()과 비슷한 선언. (...)\n",
    "        n = 0\n",
    "        loss = 0.\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            n += images.size(0)\n",
    "\n",
    "            if training_options['adversarial_training']:\n",
    "                adv_opts = AdversarialOptions(\n",
    "                        use_l2min=training_options['l2_min'],\n",
    "                        eps=0.1 if training_options['l2_min'] else 0.01,\n",
    "                        use_half_and_half=training_options['robust_additions'],\n",
    "                        ensure_proper_minimization=False,\n",
    "                )\n",
    "                \n",
    "                ##############################메인\n",
    "                \n",
    "                images = _adv_images(m, images, labels, adv_opts) \n",
    "                \n",
    "                ########################################\n",
    "            stats = _model_train_batch(m, training_options, opt, images,\n",
    "                    labels)\n",
    "            loss += stats['class_loss_sum']\n",
    "            if ONE_BATCH_ONLY:\n",
    "                break\n",
    "        print(f'Loss: {loss / n:.4f}')\n",
    "        if training_options['robust_additions']:\n",
    "            print(f'Psi: {_model_robust_get_psi(training_options[\"robust_integration\"]):.1f}')\n",
    "\n",
    "        _model_save(path, m_orig, training_options)\n",
    "\n",
    "    # test 실행.\n",
    "    stats = _model_evaluate(m, test_loader)\n",
    "    print(\"Testing statistics:\")\n",
    "    for k, v in stats.items():\n",
    "        print(f'{k}: {v}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e1bcf",
   "metadata": {},
   "source": [
    "### 적대적이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49aca9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialOptions:\n",
    "    encourage_labels = False  # True for explanations\n",
    "    eps = 0.1 #(...)\n",
    "    eps_overshoot = 1.  # Multiplier for step size; if > 1, uses g_explain (...)\n",
    "    momentum = 0  # Momentum for generating the explanations; with a large\n",
    "                  # step count, this is useful.  Otherwise not very useful.\n",
    "    steps = 7\n",
    "    use_half_and_half = False\n",
    "    use_l2min = False\n",
    "    ensure_proper_minimization = True\n",
    "    # For explanations and ARA calculations, ensure_proper_minimization serves\n",
    "    # as a sanity check.  Basically, it checks that an adversarial example\n",
    "    # satisfying the criteria is found prior to half the number of optimization\n",
    "    # steps taken, ensuring that some number of the remaining steps are used\n",
    "    # for minimizing the perturbation.\n",
    "    \n",
    "     # 설명이나 ARA 계산을 위해서 enusre_proper_minimization은 sanity check 역할.\n",
    "    # 기본적으로, 이는 기준을 만족하는 adversarial example이 optimization steps의 절반 이전에서\n",
    "    # 발견되는 지를 체크하고, 이는 남은 steps이 perturbation을 minimizing하는 데 쓰이는 걸 확인한다.\n",
    "    # 오류방지.\n",
    "    def __init__(self, **kw):\n",
    "        for k, v in kw.items():\n",
    "            if not hasattr(AdversarialOptions, k):\n",
    "                raise ValueError(k)\n",
    "            setattr(self, k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a53e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options={'adversarial_training':True, 'l2_min':True, 'robust_additions':True}\n",
    "training_options['arch']=MODEL_ARCH\n",
    "training_options['input_offset']=MODEL_INPUT_OFFSET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3fbdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_opts=AdversarialOptions(\n",
    "                        use_l2min=training_options['l2_min'],\n",
    "                        eps=0.1 if training_options['l2_min'] else 0.01,\n",
    "                        use_half_and_half=training_options['robust_additions'],\n",
    "                        ensure_proper_minimization=False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2001834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = _get_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "901e51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=model.Model(training_options)\n",
    "g=1\n",
    "batch_size=TRAIN_BATCHSIZE*g\n",
    "opt=torch.optim.SGD(m.parameters(), lr=TRAIN_LR*g, momentum=TRAIN_MOMENTUM, weight_decay=TRAIN_WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4edf7051",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')\n",
    "m_orig=m\n",
    "if torch.cuda.device_count()>0:\n",
    "    device=torch.device('cuda')\n",
    "    m=m.to(device)\n",
    "    m=torch.nn.DataParallel(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cfaabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size,\n",
    "            shuffle=True, num_workers=8, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=8, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27e7ae5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.utils.data.dataloader.DataLoader\n"
     ]
    }
   ],
   "source": [
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aa09b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "loss=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24628ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3428336",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels =test_batch # batchsize는 128인데 images의 size는 80인 이유? 128*390 + 80 = 50000. 즉, 남는 80개부터 배치(확인완료)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de251774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "496dbdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n += images.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48257a4c",
   "metadata": {},
   "source": [
    "#### 메인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75755b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for     images = _adv_images(m, images, labels, adv_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36cc5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opts : adv_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "970d8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on AdversarialOptions in module __main__ object:\n",
      "\n",
      "class AdversarialOptions(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, **kw)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  encourage_labels = False\n",
      " |  \n",
      " |  ensure_proper_minimization = True\n",
      " |  \n",
      " |  eps = 0.1\n",
      " |  \n",
      " |  eps_overshoot = 1.0\n",
      " |  \n",
      " |  momentum = 0\n",
      " |  \n",
      " |  steps = 7\n",
      " |  \n",
      " |  use_half_and_half = False\n",
      " |  \n",
      " |  use_l2min = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(adv_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da9d72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=images.detach()\n",
    "deltas=images.new_zeros(images.size()).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99cff27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if adv_opts.use_half_and_half:\n",
    "    affected=torch.rand(images.size(0), 1, 1, 1, device=images.device)\n",
    "    affected=(affected<0.5).float()\n",
    "else:\n",
    "    affected=torch.ones(images.size(0), 1, 1, 1, device=images.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0c2a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels=labels\n",
    "target_encourage=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f65a165c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_opts.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7d99e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "size=adv_opts.eps*adv_opts.eps_overshoot\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17140c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mom=images.new_zeros(images.size()) # moment?( .., )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74356daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_opts.use_l2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45db89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_best=True if adv_opts.use_l2min else False\n",
    "extra_steps=1 if track_best else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d35adb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if track_best:\n",
    "    # Also track the first step at which results were OK, and the\n",
    "    # lowest-perturbation delta.\n",
    "    best_ok_deltas = torch.zeros_like(images) # 이미지 텐서와 같은 0 tensor \n",
    "    # batch size의 0 tensor.. \n",
    "    best_ok_deltas_sqr = images.new_zeros(images.size(0)).fill_(1e30) # size로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac6f4c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a58a3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses=m(images+deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6198cf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "print(guesses.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa5bd17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0266e-01,  4.9181e-01,  6.8617e-03,  4.8616e-03,  1.3202e-01,\n",
       "         1.0858e+00,  1.0398e-01,  9.8926e-01,  3.3431e-01, -1.0808e-03],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fd83bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0885, 0.1069, 0.0658, 0.0657, 0.0746, 0.1936, 0.0725, 0.1758, 0.0913,\n",
       "        0.0653], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(guesses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d062346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0266e-01,  4.9181e-01,  6.8617e-03,  4.8616e-03,  1.3202e-01,\n",
       "         1.0858e+00,  1.0398e-01,  9.8926e-01,  3.3431e-01, -1.0808e-03],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72c8e2bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9, 5, 6, 5, 0, 3, 2, 1, 5, 8, 8, 9, 3, 5, 0, 9, 2, 0, 4, 8, 3, 4,\n",
       "        3, 0, 6, 4, 5, 2, 3, 5, 4, 2, 3, 1, 6, 3, 1, 6, 7, 7, 8, 1, 4, 2, 8, 3,\n",
       "        6, 1, 9, 9, 0, 1, 0, 8, 1, 3, 3, 3, 0, 9, 7, 3, 3, 7, 3, 1, 4, 3, 8, 7,\n",
       "        7, 2, 0, 2, 6, 6, 4, 6, 9, 9, 7, 8, 0, 7, 7, 5, 6, 5, 3, 1, 6, 7, 4, 2,\n",
       "        7, 3, 6, 2, 6, 2, 4, 7, 6, 9, 4, 7, 1, 7, 5, 9, 7, 8, 8, 2, 4, 2, 1, 9,\n",
       "        2, 9, 3, 2, 4, 7, 7, 6], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdcefd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(guesses, target_labels,\n",
    "                reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cffe4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grads=torch.autograd.grad(loss.sum(), deltas, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "558b5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grads=image_grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1bb685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(adv_opts.use_l2min, str):\n",
    "    if adv_opts.use_l2min == 'btr':\n",
    "        sm = guesses.softmax(1)\n",
    "        follow_loss = (sm.gather(1, target_labels.unsqueeze(1))[:, 0]\n",
    "                > 1/sm.size(1)).float()\n",
    "    else:\n",
    "        raise NotImplementedError(adv_opts.use_l2min)\n",
    "        \n",
    "elif adv_opts.use_l2min:\n",
    "            # l2_min; aim to be correct\n",
    "            follow_loss = (guesses.argmax(1) == target_labels).float()\n",
    "else:\n",
    "    # Standard l2; aim to be within eps.\n",
    "    follow_loss = (deltas.detach().pow(2).mean((1, 2, 3))\n",
    "            < adv_opts.eps ** 2).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "853e1b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측이 맞은 친구들. 후에 (... )\n",
    "follow_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5197552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if track_best: # l2min 사용할 때.\n",
    "    # Track the best perturbation which satisfies the criteria.\n",
    "    ddist = deltas.detach().pow(2).mean((1, 2, 3)) # noise의 L2 mean. (channel, w, h)\n",
    "    \n",
    "    # uncorrect sample  +  noise(delta)가 의 l2 mean이 best 일 때. 즉, 작을 때.\n",
    "    set_best = (follow_loss == 0) * (ddist < best_ok_deltas_sqr) # 예측을 못한 경우\n",
    "    \n",
    "    # 예측도 틀리고, L2 mean도 작은 delta들만 best deltas로 보낸다. \n",
    "    best_ok_deltas[set_best] = deltas.detach()[set_best]\n",
    "    best_ok_deltas_sqr[set_best] = ddist[set_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f44f5911",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-67-b7e2bbeb5996>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-b7e2bbeb5996>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2b5ffa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encourage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "677ce998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000000000000001"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size*(adv_opts.steps-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a328be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51bfe8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=size*(adv_opts.steps -step) / (adv_opts.steps*(adv_opts.steps+1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7a5ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_loss=follow_loss.view(-1,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d7a17077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]]],\n",
       "\n",
       "\n",
       "        [[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]]],\n",
       "\n",
       "\n",
       "        [[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]]],\n",
       "\n",
       "\n",
       "        [[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]]],\n",
       "\n",
       "\n",
       "        [[[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]],\n",
       "\n",
       "         [[-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          ...,\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.],\n",
       "          [-0., -0., -0.,  ..., -0., -0., -0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-deltas.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "297bbbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[0.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]],\n",
       "\n",
       "\n",
       "        [[[1.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-follow_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a3a2f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          ...,\n",
       "          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          ...,\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          ...,\n",
       "          [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.0015,  0.0089,  0.0138,  ..., -0.0092, -0.0033,  0.0001],\n",
       "          [ 0.0055,  0.0147,  0.0054,  ...,  0.0309, -0.0049,  0.0007],\n",
       "          [-0.0031,  0.0048,  0.0090,  ..., -0.0115,  0.0174,  0.0028],\n",
       "          ...,\n",
       "          [-0.0046, -0.0165,  0.0099,  ..., -0.0035,  0.0066, -0.0091],\n",
       "          [ 0.0130,  0.0019,  0.0011,  ...,  0.0021, -0.0027,  0.0153],\n",
       "          [-0.0030,  0.0134,  0.0046,  ...,  0.0112, -0.0031, -0.0036]],\n",
       "\n",
       "         [[-0.0027, -0.0078,  0.0281,  ...,  0.0034,  0.0109, -0.0004],\n",
       "          [-0.0132,  0.0153,  0.0221,  ..., -0.0096,  0.0153, -0.0112],\n",
       "          [-0.0103, -0.0060, -0.0179,  ...,  0.0136, -0.0201,  0.0070],\n",
       "          ...,\n",
       "          [ 0.0044, -0.0068, -0.0097,  ...,  0.0025, -0.0017, -0.0271],\n",
       "          [ 0.0066,  0.0037,  0.0090,  ..., -0.0005,  0.0028,  0.0077],\n",
       "          [-0.0011, -0.0040,  0.0035,  ...,  0.0073,  0.0064, -0.0075]],\n",
       "\n",
       "         [[-0.0111, -0.0078,  0.0009,  ...,  0.0016, -0.0057,  0.0049],\n",
       "          [ 0.0114, -0.0120, -0.0234,  ..., -0.0051,  0.0064,  0.0031],\n",
       "          [-0.0063, -0.0019,  0.0404,  ..., -0.0404, -0.0257,  0.0195],\n",
       "          ...,\n",
       "          [ 0.0017, -0.0035, -0.0056,  ...,  0.0064, -0.0106,  0.0002],\n",
       "          [-0.0113, -0.0160, -0.0054,  ...,  0.0031, -0.0008, -0.0047],\n",
       "          [-0.0012, -0.0061,  0.0056,  ...,  0.0028, -0.0098,  0.0063]]],\n",
       "\n",
       "\n",
       "        [[[-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          ...,\n",
       "          [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          ...,\n",
       "          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          ...,\n",
       "          [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "          ...,\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          ...,\n",
       "          [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0000,  0.0000]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdir=follow_loss*image_grads+(1-follow_loss)*-deltas.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bbbdf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdir = (\n",
    "                follow_loss * image_grads\n",
    "                + (1 - follow_loss) * -deltas.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146d433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee4989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58d38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6a665b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8031, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55456474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "434bad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2359, 2.3612, 2.3073, 2.0079, 2.2287, 2.1392, 2.3744, 2.6745, 2.3502,\n",
       "        2.0370, 2.0248, 2.5069, 2.5007, 3.6208, 2.2783, 2.1875, 2.0193, 2.6752,\n",
       "        3.0164, 2.2428, 2.7278, 2.4528, 3.0654, 2.6148, 2.8551, 2.7972, 2.2589,\n",
       "        2.2446, 2.0398, 4.0249, 2.5222, 2.1833, 2.4737, 2.4336, 2.5988, 2.1357,\n",
       "        2.4553, 2.6813, 2.0680, 2.5388, 1.9658, 2.0778, 2.4983, 2.2687, 2.5078,\n",
       "        3.0193, 2.4752, 2.6348, 2.4686, 2.0236, 2.6285, 2.6880, 2.1875, 2.2762,\n",
       "        2.0067, 2.6311, 2.1178, 2.4885, 2.9132, 2.2765, 1.9918, 2.6611, 2.0264,\n",
       "        3.0685, 2.6208, 2.2330, 2.6871, 1.9551, 2.2948, 3.0446, 2.6085, 2.0972,\n",
       "        2.1306, 2.6930, 2.1192, 2.4014, 2.2955, 2.3346, 2.6664, 2.1023, 2.7488,\n",
       "        2.6142, 1.9777, 2.2870, 2.2871, 2.1731, 1.9677, 2.2774, 2.0741, 2.1110,\n",
       "        3.0453, 2.0682, 2.6141, 2.1982, 2.4420, 2.4664, 2.1319, 2.5967, 2.3989,\n",
       "        3.1402, 2.2477, 2.8931, 2.5287, 2.0738, 2.2899, 2.3590, 2.4865, 2.0801,\n",
       "        2.5266, 2.0636, 1.8821, 2.5005, 2.0633, 2.3880, 2.7782, 2.4438, 2.4132,\n",
       "        3.1149, 2.0780, 3.0880, 2.4202, 2.7177, 2.8925, 2.3297, 2.5263, 1.9486,\n",
       "        2.1389, 2.1332], device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(guesses, target_labels, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels : gpu에 tensor단위로 할당된 상태\n",
    "# 아래 함수는 하나의 batch 내에서 실행\n",
    "\n",
    "def _adv_images(m, images, labels, opts):\n",
    "    if opts.steps <= 0:\n",
    "        return images\n",
    "\n",
    "    images = images.detach() # 복사. images.data()와 동일하나 오류 측면 처리에서 선호, \n",
    "    deltas = images.new_zeros(images.size()).requires_grad_() # gradient 자동 추적, 하고싶지 않을 시 detach. --> 학습용\n",
    "    \n",
    "    # affected : [batch_size, 1, 1, 1] 크기의 binary tensor\n",
    "    # affected : ( ... ) 의 역할\n",
    "    if opts.use_half_and_half: # batch가 절반의 adversarial, 절반의 original로 구성\n",
    "        # 배치 사이즈 만큼의 tensor를 random으로 생성해 약 절반을 0으로 조정\n",
    "        affected = torch.rand(images.size(0), 1, 1, 1, device=images.device)\n",
    "        affected = (affected < 0.5).float()\n",
    "    else:\n",
    "        affected = torch.ones(images.size(0), 1, 1, 1, device=images.device)\n",
    "    \n",
    "    target_labels = labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 설명을 위해서는 True로 설정, 아마 학습에는 False\n",
    "    target_encourage = False \n",
    "    if opts.encourage_labels:\n",
    "        if isinstance(target_labels, list): # 혹시 label이 list형태일 경우\n",
    "            target_labels = torch.LongTensor(target_labels).to(images.device) # LongTensor는 정수형으로 나타난다.\n",
    "        target_encourage = True\n",
    "        \n",
    "        # l2min이랑 labels에대한 설명은 동시에 할 수 없다. (왜? ...) 학습이라? \n",
    "        assert not opts.use_l2min, 'Cannot combine l2_min with encourage_labels'\n",
    "    \n",
    "    # size는 ( step size에 곱해지는 데)에 적용. (더 민감하게 이동) =0.1 \n",
    "    size = opts.eps * opts.eps_overshoot # 0.1 (eps)* 1 ( # Multiplier for step size; if > 1, uses g_explain)\n",
    "    mom = images.new_zeros(images.size()) # momemntum, (step size가 클 때. )\n",
    "    \n",
    "    # l2min을 사용할 때 track_best = True, extra_steps =1\n",
    "    track_best = True if opts.use_l2min else False\n",
    "    extra_steps = 1 if track_best else 0\n",
    "    \n",
    "    # ARA 계산 또는 Explanation에 해당한다면 (...) Explanation 할 때.\n",
    "    if opts.ensure_proper_minimization: \n",
    "        first_ok_steps = images.new_zeros(images.size(0),\n",
    "                dtype=torch.int).fill_(999 + opts.steps)\n",
    "    \n",
    "    # l2min을 사용할 때\n",
    "    if track_best:\n",
    "        # Also track the first step at which results were OK, and the lowest-perturbation delta.\n",
    "        # 결과가 양호했던 first step과 lowest-perturbation delta를 추적합니다.\n",
    "        # (noise가 l2min 해야하나? ... )\n",
    "        best_ok_deltas = torch.zeros_like(images) # 이미지 텐서와 같은 0 tensor \n",
    "        \n",
    "        # batch size 크기의 0 tensor. (sqr : ( delta의 L2 mean 값과 비교된다.))\n",
    "        best_ok_deltas_sqr = images.new_zeros(images.size(0)).fill_(1e30)\n",
    "\n",
    "        \n",
    "    for step in range(opts.steps + extra_steps): # default (6 + 1)번\n",
    "        \n",
    "        # guesses는 softmax 이전의 값(-1.8, 1.8 등의 값도 나온다.)\n",
    "        # pytorch의 cross_entropy는 softmax를 포함한 것으로 보인다. \n",
    "        guesses = m(images + deltas) # noise image를 받아 pre-softmax.\n",
    "        loss = torch.nn.functional.cross_entropy(guesses, target_labels,\n",
    "                reduction='none') # reduction : 모든 batch loss를 mean 할거냐, sum 할거냐..\n",
    "\n",
    "        # detach() not necessary, but just in case it changes...\n",
    "        # loss 대비 noise(delta)의 gradient 구하기\n",
    "\n",
    "        image_grads = torch.autograd.grad(loss.sum(), deltas)[0].detach()\n",
    "\n",
    "        # ( ... 쓰이는 부분 있는지)\n",
    "        if isinstance(opts.use_l2min, str): # 어떻게 str가 나오지? False 아님 True인데. \n",
    "            print('adv_images, isinstance(opts.use_l2min ,str) 발동.')\n",
    "            if opts.use_l2min == 'btr':\n",
    "                sm = guesses.softmax(1)\n",
    "                follow_loss = (sm.gather(1, target_labels.unsqueeze(1))[:, 0]\n",
    "                        > 1/sm.size(1)).float()\n",
    "            else:\n",
    "                raise NotImplementedError(opts.use_l2min)\n",
    "        elif opts.use_l2min: # l2min=True 일 때, \n",
    "            # l2_min; aim to be correct(네트워크가 올바르게 분류해야 흐른다)\n",
    "            # 올바르지 못하게 분류한다면, 정 반대로 대체할 것.\n",
    "            # batch size의 binary tensor로 나타냄. (정답 example)\n",
    "            follow_loss = (guesses.argmax(1) == target_labels).float() # argmax(1) : row-dirrection\n",
    "\n",
    "        # follow_loss는 후에 ( ... ) 하는 데 쓰인다. \n",
    "\n",
    "        else: # l2min=False일 때.\n",
    "            # Standard l2; aim to be within eps. highest loss를 찾는다. \n",
    "            # channel, width, height의 평균을 구해 batch size의 평균을 얻는다.\n",
    "            # 특히, (noise)^2 - (epsilon)^2 < 0 인 친구들을 찾는다. \n",
    "            # batch size의 binary tensor로 나타남. (epsilon보다 작은 친구들.)\n",
    "            follow_loss = (deltas.detach().pow(2).mean((1, 2, 3))\n",
    "                    < opts.eps ** 2).float()\n",
    "\n",
    "        if opts.ensure_proper_minimization: # 설명 또는 ARA 구할 때만. \n",
    "            # Note that follow_loss == 0 checks for the boundary condition\n",
    "            # being satisfied.\n",
    "            is_ok = (follow_loss == 0)\n",
    "            set_step = is_ok * (first_ok_steps > step)\n",
    "            first_ok_steps[set_step] = step\n",
    "\n",
    "        if track_best: # l2min 사용할 때.\n",
    "            # Track the best perturbation which satisfies the criteria.\n",
    "            # ddist : noise의 l2 mean. \n",
    "            ddist = deltas.detach().pow(2).mean((1, 2, 3))\n",
    "\n",
    "            # uncorrect sample  +  noise(delta)가 의 l2 mean이 best 일 때. 즉, 작을 때.\n",
    "            set_best = (follow_loss == 0) * (ddist < best_ok_deltas_sqr)\n",
    "            \n",
    "            # l2 mean이 작으면서 예측 또한 못하는 noise가 좋은 noise이다.\n",
    "            best_ok_deltas[set_best] = deltas.detach()[set_best]\n",
    "            best_ok_deltas_sqr[set_best] = ddist[set_best]\n",
    "\n",
    "            # All done; are we in extra step?  If so, don't do another step\n",
    "            if step == opts.steps:\n",
    "                deltas = best_ok_deltas\n",
    "                break\n",
    "\n",
    "        if target_encourage:\n",
    "            image_grads *= -1\n",
    "\n",
    "        # Let the GC clean up, if needed\n",
    "        guesses = None\n",
    "        loss = None\n",
    "\n",
    "        # Ramp step size - 점점 step size를 작게 잡는다., 논문 Method 참고\n",
    "        ss = size * (opts.steps - step) / (opts.steps * (opts.steps + 1) / 2)\n",
    "\n",
    "        # Find direction and follow it an amount that increases RMSE up to ss\n",
    "        follow_loss = follow_loss.view(-1, 1, 1, 1) # [128] -> [128, 1, 1, 1]\n",
    "\n",
    "\n",
    "        ### sdir : ( ... )\n",
    "        sdir = (\n",
    "                follow_loss * image_grads # 제대로 예측한 친구는 delta의 gradient. \n",
    "                + (1 - follow_loss) * -deltas.detach()) # follow_loss : l2min에서 올바르게 분류한 sample에 해당한다. \n",
    "        # 즉, (1 - follow_loss) 는 제대로 예측하지 못한 sample이다. 제대로 예측한 sample은 0으로 영향 x \n",
    "\n",
    "        # normalization이라 보면 될 듯. 3차원 이미지의 평균값으로 나눠버림(L2 mean), 단 sample은 sample끼리, channel, w, h만 나눠짐.\n",
    "        sdir /= 1e-8 + sdir.pow(2).mean((1, 2, 3)).sqrt().view(-1, 1, 1, 1)\n",
    "        # Momentum helps when steps are high (450), hurts when steps are\n",
    "        # low (45).  Difference is not necessarily significant, however.\n",
    "\n",
    "        # 기본값은 0이다. \n",
    "        # sdir : [batch, 3, 32, 32] 차원의 (예측 잘한 sample의 delta's gradient - 예측 못한 sample의 delta 곱)\n",
    "        if opts.momentum > 0:\n",
    "            mom = mom.mul_(opts.momentum).add_(1 - opts.momentum, sdir)\n",
    "            sdir = mom.clone()\n",
    "\n",
    "        # affected : half and half training 시 랜덤하게 약 절반만 영향을 끼침.\n",
    "        sdir *= affected\n",
    "        # step size만큼 곱해서 sdir을 delta, 즉 noise에 더한다. \n",
    "        deltas.data.add_(ss, sdir)        \n",
    "\n",
    "    \n",
    "    # 설명할 때. \n",
    "    if opts.ensure_proper_minimization:\n",
    "        if (first_ok_steps > opts.steps // 2).sum().item() != 0:\n",
    "            raise ValueError(\"In order to ensure that a good bound is \"\n",
    "                    \"located, calculating ARA or generating an explanation \"\n",
    "                    \"checks that a suitable adversarial example is found \"\n",
    "                    \"within half the specified number of optimization steps.  \"\n",
    "                    \"This ensures that the reported minimal attack or \"\n",
    "                    \"explanation is sufficiently representative.  \"\n",
    "                    \"However, this was not the case.  Increase eps or steps.\")\n",
    "    \n",
    "    # step 만큼 delta(noise)의 gradient를 구하는 행위를 반복한다.\n",
    "    # 즉, 실질적으로 delta + gradient*correct - incorrect * delta. \n",
    "    return images + deltas.detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d13f7e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 32, 32])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grads.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4f973e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 2.6606e-04,  1.6321e-03,  2.5194e-03,  ..., -1.6892e-03,\n",
       "           -5.9983e-04,  2.2798e-05],\n",
       "          [ 9.9762e-04,  2.6798e-03,  9.8839e-04,  ...,  5.6446e-03,\n",
       "           -8.9418e-04,  1.2482e-04],\n",
       "          [-5.6324e-04,  8.8001e-04,  1.6518e-03,  ..., -2.0973e-03,\n",
       "            3.1737e-03,  5.0397e-04],\n",
       "          ...,\n",
       "          [-8.3305e-04, -3.0191e-03,  1.8074e-03,  ..., -6.4349e-04,\n",
       "            1.2002e-03, -1.6575e-03],\n",
       "          [ 2.3808e-03,  3.4669e-04,  1.9876e-04,  ...,  3.8215e-04,\n",
       "           -4.8633e-04,  2.7898e-03],\n",
       "          [-5.5233e-04,  2.4424e-03,  8.4650e-04,  ...,  2.0461e-03,\n",
       "           -5.7413e-04, -6.5600e-04]],\n",
       "\n",
       "         [[-4.8642e-04, -1.4253e-03,  5.1401e-03,  ...,  6.2269e-04,\n",
       "            1.9968e-03, -8.0826e-05],\n",
       "          [-2.4153e-03,  2.7953e-03,  4.0315e-03,  ..., -1.7483e-03,\n",
       "            2.7936e-03, -2.0449e-03],\n",
       "          [-1.8767e-03, -1.0959e-03, -3.2747e-03,  ...,  2.4829e-03,\n",
       "           -3.6717e-03,  1.2858e-03],\n",
       "          ...,\n",
       "          [ 8.0770e-04, -1.2456e-03, -1.7725e-03,  ...,  4.5991e-04,\n",
       "           -3.0427e-04, -4.9522e-03],\n",
       "          [ 1.2039e-03,  6.7131e-04,  1.6386e-03,  ..., -9.2896e-05,\n",
       "            5.1011e-04,  1.4088e-03],\n",
       "          [-2.0070e-04, -7.2597e-04,  6.3852e-04,  ...,  1.3300e-03,\n",
       "            1.1708e-03, -1.3686e-03]],\n",
       "\n",
       "         [[-2.0295e-03, -1.4297e-03,  1.6446e-04,  ...,  2.8613e-04,\n",
       "           -1.0391e-03,  8.9732e-04],\n",
       "          [ 2.0888e-03, -2.1905e-03, -4.2798e-03,  ..., -9.2584e-04,\n",
       "            1.1731e-03,  5.6154e-04],\n",
       "          [-1.1512e-03, -3.4128e-04,  7.3804e-03,  ..., -7.3775e-03,\n",
       "           -4.6901e-03,  3.5644e-03],\n",
       "          ...,\n",
       "          [ 3.0833e-04, -6.4036e-04, -1.0188e-03,  ...,  1.1682e-03,\n",
       "           -1.9381e-03,  2.8789e-05],\n",
       "          [-2.0618e-03, -2.9153e-03, -9.8902e-04,  ...,  5.6687e-04,\n",
       "           -1.4880e-04, -8.6195e-04],\n",
       "          [-2.1575e-04, -1.1064e-03,  1.0275e-03,  ...,  5.1484e-04,\n",
       "           -1.7974e-03,  1.1544e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]], device='cuda:0')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas.data.add(ss,sdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf68bff",
   "metadata": {},
   "source": [
    "# INPUT OFFSET?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "16c41d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_cifar_noaug():\n",
    "    \"\"\"Returns (ds_train, ds_test) with augmentations on training set.\n",
    "    \"\"\"\n",
    "    T = torchvision.transforms\n",
    "    aug_pad = 4\n",
    "    aug_dim = 32 + aug_pad * 2\n",
    "    ds_train = torchvision.datasets.CIFAR10('CIFAR10_PATH', train=True,\n",
    "            transform=T.Compose([\n",
    "                T.ToTensor(),]))\n",
    "    ds_test = torchvision.datasets.CIFAR10('CIFAR10_PATH', train=False,\n",
    "            transform=torchvision.transforms.ToTensor())\n",
    "    return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "7007cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test=get_dataset_cifar_noaug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "d5419899",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INPUT_OFFSET = [[0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "16eaff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor([0.4914])\n"
     ]
    }
   ],
   "source": [
    "total_mean=torch.Tensor([0])\n",
    "for i in range(50000):\n",
    "    total_mean=total_mean+train[i][0][0].mean()\n",
    "    #print(train[i][0][0].mean())\n",
    "print('channel0 : ', total_mean/50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "40f6458e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor([0.4822])\n"
     ]
    }
   ],
   "source": [
    "total_mean2=torch.Tensor([0])\n",
    "for i in range(50000):\n",
    "    total_mean2=total_mean2+train[i][0][1].mean()\n",
    "    #print(train[i][0][0].mean())\n",
    "print('channel0 : ', total_mean2/50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "330953c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor([0.4465])\n"
     ]
    }
   ],
   "source": [
    "total_mean3=torch.Tensor([0])\n",
    "for i in range(50000):\n",
    "    total_mean3=total_mean3+train[i][0][2].mean()\n",
    "    #print(train[i][0][0].mean())\n",
    "print('channel0 : ', total_mean3/50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "205ab16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base=torch.zeros([50000, 3, 32, 32])\n",
    "\n",
    "for i in range(50000):\n",
    "    base[i]=train[i][0]\n",
    "    if i%1000==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "34f0921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor(0.2470)\n",
      "channel1 :  tensor(0.2435)\n",
      "channel2 :  tensor(0.2616)\n"
     ]
    }
   ],
   "source": [
    "print('channel0 : ', base[:, 0, :, :].var().sqrt())\n",
    "print('channel1 : ', base[:, 1, :, :].var().sqrt())\n",
    "print('channel2 : ', base[:, 2, :, :].var().sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a77b6b",
   "metadata": {},
   "source": [
    "### 결론 : Offset은 Train과 맞춘다. 그런데, 이는 다른 dataset도 얼마 차이 안 날텐데;;  \n",
    "[[0.3006, 0.2956, 0.2926], [0.2196, 0.2133, 0.2098]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0.3006, 0.2956, 0.2926], [0.2196, 0.2133, 0.2098]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "1d6410ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test=_get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "6f3a0aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 133641\n",
       "    Root location: ../BDD100K_MOT2020_image/bdd100k/images/track/train_av\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "e867b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n"
     ]
    }
   ],
   "source": [
    "base_av=torch.zeros([133641, 3, 32, 32])\n",
    "\n",
    "for i in range(133641):\n",
    "    base_av[i]=ds_train[i][0]\n",
    "    if i%5000==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "fd4d16bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor(0.3006)\n",
      "channel1 :  tensor(0.2956)\n",
      "channel2 :  tensor(0.2926)\n"
     ]
    }
   ],
   "source": [
    "print('channel0 : ', base_av[:, 0, :, :].mean())\n",
    "print('channel1 : ', base_av[:, 1, :, :].mean())\n",
    "print('channel2 : ', base_av[:, 2, :, :].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "c4914041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor(0.2196)\n",
      "channel1 :  tensor(0.2133)\n",
      "channel2 :  tensor(0.2098)\n"
     ]
    }
   ],
   "source": [
    "print('channel0 : ', base_av[:, 0, :, :].var().sqrt())\n",
    "print('channel1 : ', base_av[:, 1, :, :].var().sqrt())\n",
    "print('channel2 : ', base_av[:, 2, :, :].var().sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ddf935",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0.3006, 0.2956, 0.2926], [0.2196, 0.2133, 0.2098]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6278582",
   "metadata": {},
   "source": [
    "# 0610(Offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34074d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root='../BDD100K_MOT2020_image/bdd100k/images/track/final_train_av' # 0610\n",
    "test_root='../BDD100K_MOT2020_image/bdd100k/images/track/final_test_av' # 0610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a817e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset():\n",
    "    \n",
    "    \n",
    "    \"\"\"Returns (ds_train, ds_test) with augmentations on training set.\n",
    "    \"\"\"\n",
    "    T = torchvision.transforms\n",
    "        \n",
    "    ds_train = torchvision.datasets.ImageFolder(train_root, \n",
    "            transform=T.ToTensor())\n",
    "    ds_test = torchvision.datasets.ImageFolder(test_root, \n",
    "            transform=T.ToTensor())\n",
    "    \n",
    "    return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "408c94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test=_get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00943701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0571466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b491893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35cecd68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'bicycle',\n",
       " 'bus',\n",
       " 'car',\n",
       " 'motorcycle',\n",
       " 'other person',\n",
       " 'other vehicle',\n",
       " 'pedestrian',\n",
       " 'rider',\n",
       " 'truck']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad7e4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 90000\n",
       "    Root location: ../BDD100K_MOT2020_image/bdd100k/images/track/final_train_av\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b657e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8faed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 processed .. \n",
      "10000 processed .. \n",
      "20000 processed .. \n",
      "30000 processed .. \n",
      "40000 processed .. \n",
      "50000 processed .. \n",
      "60000 processed .. \n",
      "70000 processed .. \n",
      "80000 processed .. \n"
     ]
    }
   ],
   "source": [
    "base_av=torch.zeros([90000, 3, 32, 32])\n",
    "\n",
    "for i in range(90000):\n",
    "    base_av[i]=ds_train[i][0]\n",
    "    if i%10000==0:\n",
    "        print(i, 'processed .. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27cea5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor(0.3040)\n",
      "channel1 :  tensor(0.3043)\n",
      "channel2 :  tensor(0.2983)\n"
     ]
    }
   ],
   "source": [
    "print('channel0 : ', base_av[:, 0, :, :].mean())\n",
    "print('channel1 : ', base_av[:, 1, :, :].mean())\n",
    "print('channel2 : ', base_av[:, 2, :, :].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36deff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel0 :  tensor(0.2154)\n",
      "channel1 :  tensor(0.2110)\n",
      "channel2 :  tensor(0.2051)\n"
     ]
    }
   ],
   "source": [
    "print('channel0 : ', base_av[:, 0, :, :].var().sqrt())\n",
    "print('channel1 : ', base_av[:, 1, :, :].var().sqrt())\n",
    "print('channel2 : ', base_av[:, 2, :, :].var().sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0.3040, 0.3043, 0.2983], [0.2154, 0.2110, 0.2051]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba29e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cbd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1f1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1926f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b07cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1d92555",
   "metadata": {},
   "source": [
    "## 기타함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe6eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        x = x - self.input_offset[0].view(1, -1, 1, 1)\n",
    "        x /= self.input_offset[1].view(1, -1, 1, 1)\n",
    "        x = self.pipe(x)\n",
    "        assert x.size(2) == 1 and x.size(3) == 1, x.size()\n",
    "        return x[:, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# fix torch random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class histoCancerDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform, data_type='train'):\n",
    "        # path to images\n",
    "        path2data = os.path.join(data_dir, data_type)\n",
    "\n",
    "        # get a list of images\n",
    "        filenames = os.listdir(path2data)\n",
    "\n",
    "        # get the full path to images\n",
    "        self.full_filenames = [os.path.join(path2data, f) for f in filenames]\n",
    "\n",
    "        # labels are in a csv file named train_labels.csv\n",
    "        csv_filename = data_type + '_labels.csv'\n",
    "        path2csvLabels = os.path.join(data_dir, csv_filename)\n",
    "        labels_df = pd.read_csv(path2csvLabels)\n",
    "\n",
    "        # set data frame index to id\n",
    "        labels_df.set_index('id', inplace=True)\n",
    "\n",
    "        # obtain labels from data frame\n",
    "        self.labels = [labels_df.loc[filename[:-4]].values[0] for filename in filenames]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return size of dataset\n",
    "        return len(self.full_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # open image, apply transforms and return with label\n",
    "        image = Image.open(self.full_filenames[idx])\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# define a simple transformation that only converts a PIL image into PyTorch tensors\n",
    "import torchvision.transforms as transforms\n",
    "data_transformer = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# define an object of the custom dataset for the train folder\n",
    "data_dir ='/content/cookbook/MyDrive/data'\n",
    "histo_dataset = histoCancerDataset(data_dir, data_transformer, 'train')\n",
    "\n",
    "print(len(histo_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b945f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f69db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cedbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset():\n",
    "    \"\"\"Returns (ds_train, ds_test) with augmentations on training set.\n",
    "    \"\"\"\n",
    "    T = torchvision.transforms\n",
    "    aug_pad = 4\n",
    "    aug_dim = 32 + aug_pad * 2\n",
    "    ds_train = torchvision.datasets.CIFAR10(cifar10_path, train=True,\n",
    "            transform=T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Lambda(lambda tensor:\n",
    "                    torch.nn.functional.pad(\n",
    "                        tensor.view(1, 3, 32, 32),\n",
    "                        (aug_pad,)*4,\n",
    "                        'replicate').view(3, aug_dim, aug_dim)),\n",
    "                T.ToPILImage(),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomCrop((32, 32)),\n",
    "                T.ToTensor(),\n",
    "            ]))\n",
    "    ds_test = torchvision.datasets.CIFAR10(cifar10_path, train=False,\n",
    "            transform=torchvision.transforms.ToTensor())\n",
    "    return ds_train, ds_test\n",
    "\n",
    "\n",
    "def _model_evaluate(m, ds_loader):\n",
    "    device = next(m.parameters()).device\n",
    "    m.eval()\n",
    "    stats = {'accuracy': 0.}\n",
    "    n = 0\n",
    "    for batch in ds_loader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        n += images.size(0)\n",
    "\n",
    "        preds = m(images)\n",
    "        preds = preds.argmax(1)\n",
    "        stats['accuracy'] += (preds == labels).float().sum().item()\n",
    "\n",
    "        if ONE_BATCH_ONLY:\n",
    "            break\n",
    "    stats['accuracy'] /= n\n",
    "    return stats\n",
    "\n",
    "\n",
    "def _model_robust_get_grads(guesses, images, labels, train):\n",
    "    \"\"\"Get the gradients of guesses with respect to images, according to K=1\n",
    "    and ROBUST_ZETA.\n",
    "\n",
    "    ``train`` specifies whether or not the model is in training mode; if it is,\n",
    "    the gradient tree will be retained.\n",
    "    \"\"\"\n",
    "    to_smooth = None  # Output, or combination of outputs, for gradient\n",
    "\n",
    "    # Select randomly smoothed nodes\n",
    "    smg = torch.rand(guesses.size(0), guesses.size(1),\n",
    "            device=guesses.device)\n",
    "    smg = smg.argmax(1)\n",
    "    sfair = guesses.gather(1, smg.unsqueeze(1))[:, 0]\n",
    "\n",
    "    # Determine if any should be smoothed in tandem\n",
    "    if ROBUST_ZETA > 0:\n",
    "        # Determine which, if any, should be unfair\n",
    "        unfair = (torch.rand(guesses.size(0), device=guesses.device)\n",
    "                < ROBUST_ZETA).float()\n",
    "\n",
    "        # Select unfairly smoothed\n",
    "        unfair_real = guesses.gather(1, labels.unsqueeze(1))[:, 0]\n",
    "        unfair_max = guesses.clone()\n",
    "        unfair_max.scatter_(1, labels.unsqueeze(1),\n",
    "                unfair_max.detach().min() - 1)\n",
    "        sunfair = (unfair_real - unfair_max.max(1)[0])\n",
    "\n",
    "        to_smooth = unfair * sunfair + (1 - unfair) * sfair\n",
    "    else:\n",
    "        # Smooth only single outputs\n",
    "        to_smooth = sfair\n",
    "\n",
    "    # Calculate gradients of input with respect to to_smooth\n",
    "    grads = torch.autograd.grad(to_smooth.sum(), images, create_graph=train)[0]\n",
    "    return grads\n",
    "\n",
    "\n",
    "def _model_robust_get_psi(robust_integration):\n",
    "    return ROBUST_ADAPT_PSI_0 * math.exp(ROBUST_ADAPT_PSI * robust_integration)\n",
    "\n",
    "\n",
    "def _model_save(path, m, training_options):\n",
    "    torch.save({'model_params': m.state_dict(),\n",
    "            'training_options': training_options}, path)\n",
    "\n",
    "\n",
    "def _model_load(path):\n",
    "    d = torch.load(path)\n",
    "    m = model.Model(d['training_options'])\n",
    "    m.load_state_dict(d['model_params'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896af46f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
