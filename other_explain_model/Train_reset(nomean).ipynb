{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd1e47d",
   "metadata": {},
   "source": [
    "> cuDNN 오류해결?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660b362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf \n",
    " \n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     # 텐서플로가 세 번째 GPU만 사용하도록 제한\n",
    "#     try:\n",
    "#         tf.config.experimental.set_visible_devices(gpus[5], 'GPU')\n",
    "#     except RuntimeError as e:\n",
    "#         # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f059cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # 위에껄로 대체가능\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f268a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11304472305965390561,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 31594145408\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12003164023120331230\n",
       " physical_device_desc: \"device: 0, name: NVIDIA Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af097f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79713805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa079bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # #싱글지피유인데 오류가뜬다면?\n",
    "\n",
    "# # gpus = tf.config.experimental.lis\n",
    "# t_physical_devices('GPU')\n",
    "# # if gpus:\n",
    "# #     try:\n",
    "# #         # Currently, memory growth needs to be the same across GPUs\n",
    "# #         for gpu in gpus:\n",
    "# #             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# #         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "# #         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "# #     except RuntimeError as e:\n",
    "# #         # Memory growth must be set before GPUs have been initialized\n",
    "# #         print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80419bd0",
   "metadata": {},
   "source": [
    "# 기타 퍼스널 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38888256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_numpy(train_ds, test_ds):\n",
    "\n",
    "    # av dataset\n",
    "    # 0610 : 9개의 클래스에 대해 train(!0000), test(3000)\n",
    "    x_train_base=np.zeros([90000, 32, 32, 3])\n",
    "    y_train_base=np.zeros([90000, 1])\n",
    "    x_test_base=np.zeros([27000, 32, 32, 3])\n",
    "    y_test_base=np.zeros([27000, 1])\n",
    "\n",
    "    numpy_iter=list(train_ds.as_numpy_iterator())\n",
    "\n",
    "    for idx, batch in enumerate(numpy_iter):\n",
    "    #     if idx%30==0:\n",
    "    #         print(idx)\n",
    "        x_train_base[128*idx:128*(idx+1), :, :, :]=batch[0]\n",
    "        y_train_base[128*idx:128*(idx+1), 0]=batch[1]\n",
    "\n",
    "    numpy_iter_test=list(test_ds.as_numpy_iterator())\n",
    "\n",
    "    len(numpy_iter_test)\n",
    "\n",
    "    for idx, batch in enumerate(numpy_iter_test):\n",
    "    #     if idx%30==0:\n",
    "    #         print(idx)\n",
    "        x_test_base[128*idx:128*(idx+1), :, :, :]=batch[0]\n",
    "        y_test_base[128*idx:128*(idx+1), 0]=batch[1]\n",
    "    \n",
    "    return x_train_base, y_train_base, x_test_base, y_test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a704b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    train_dir='../BDD100K_MOT2020_image/bdd100k/images/track/final_train_av'\n",
    "    test_dir='../BDD100K_MOT2020_image/bdd100k/images/track/final_test_av'\n",
    "\n",
    "    batch_size = 128\n",
    "    img_height = 32\n",
    "    img_width = 32\n",
    "\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      train_dir,\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      test_dir,\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da8642",
   "metadata": {},
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bd10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a ResNet on the CIFAR10 dataset.\n",
    "ResNet v1\n",
    "[a] Deep Residual Learning for Image Recognition\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "ResNet v2\n",
    "[b] Identity Mappings in Deep Residual Networks\n",
    "https://arxiv.org/pdf/1603.05027.pdf\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input\n",
    "from tensorflow.keras.layers import Flatten, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10 # \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "# training parameters\n",
    "batch_size = 128 # orig paper trained all networks with batch_size=128\n",
    "epochs = 200\n",
    "data_augmentation = False # 0610 - pytorch와 통합\n",
    "num_classes = 9 # 0604, 0610 - (9개)\n",
    "\n",
    "# subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = False #0616 False)\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 7 # --  REsnet 44\n",
    "\n",
    "# model version\n",
    "# orig paper: version = 1 (ResNet v1), \n",
    "# improved ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "# model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# load the CIFAR10 data.\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6940a0",
   "metadata": {},
   "source": [
    "> Trainset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bd1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.load('x_train.npy')\n",
    "x_test=np.load('x_test.npy')\n",
    "y_train=np.load('y_train.npy')\n",
    "y_test=np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "530b1951",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90000 files belonging to 9 classes.\n",
      "Found 27000 files belonging to 9 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-598c656fc36b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 0604\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#0604\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-bdcbebf487c3>\u001b[0m in \u001b[0;36mbatch_to_numpy\u001b[0;34m(train_ds, test_ds)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_test_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m27000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnumpy_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3941\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3942\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3944\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2574\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2575\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2577\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_ds, test_ds=get_dataset() # 0604\n",
    "# x_train, y_train, x_test, y_test=batch_to_numpy(train_ds, test_ds) #0604"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f9b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape :  (32, 32, 3)\n",
      "x_train shape: (90000, 32, 32, 3)\n",
      "90000 train samples\n",
      "27000 test samples\n",
      "y_train shape: (90000, 1)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "print('input_shape : ', input_shape)\n",
    "# normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_train=y_train.astype('uint8')\n",
    "y_test=y_test.astype('uint8')\n",
    "# if subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# convert class vectors to binary class matrices.\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a2ad051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    Arguments:\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    Returns:\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=num_classes):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved\n",
    "    (downsampled) by a convolutional layer with strides=2, while \n",
    "    the number of filters is doubled. Within each stage, \n",
    "    the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    Arguments:\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    Returns:\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n",
    "    # start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:\n",
    "                # linear projection residual shortcut\n",
    "                # connection to match changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afffb978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d96cb6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 16)   0           activation_10[0][0]              \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 16)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2320        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 16)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2320        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 16)   0           activation_12[0][0]              \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 16)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 32)   4640        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 32)   9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   544         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 32)   0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 32)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 32)   128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 32)   0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 32)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   9248        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 32)   0           activation_18[0][0]              \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 32)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9248        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9248        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 32)   0           activation_20[0][0]              \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 32)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 32)   128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 32)   0           activation_22[0][0]              \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 32)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 32)   9248        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 32)   128         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 32)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9248        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 32)   0           activation_24[0][0]              \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 32)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 32)   9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 32)   9248        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 32)   0           activation_26[0][0]              \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 32)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 64)     18496       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 64)     256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 64)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 64)     2112        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 64)     0           conv2d_32[0][0]                  \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 64)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 64)     36928       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 64)     256         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 64)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 64)     36928       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 64)     256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 64)     0           activation_30[0][0]              \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 64)     0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 64)     36928       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 64)     256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 64)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 64)     36928       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 64)     256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 64)     0           activation_32[0][0]              \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 64)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 64)     36928       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 64)     36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 64)     0           activation_34[0][0]              \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 64)     0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 64)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 64)     36928       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 8, 8, 64)     0           activation_36[0][0]              \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 64)     0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 64)     256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36928       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 64)     0           activation_38[0][0]              \n",
      "                                                                 batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 64)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 64)     36928       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 64)     256         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 64)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     36928       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 8, 8, 64)     0           activation_40[0][0]              \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 64)     0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 9)            585         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 665,929\n",
      "Trainable params: 662,761\n",
      "Non-trainable params: 3,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)), #0610 변경 lr -> leraning_Rate\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92389035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet44v1\n",
      "Not using data augmentation.\n",
      "Epoch 1/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 50s 52ms/step - loss: 2.0684 - acc: 0.3757 - val_loss: 2.2026 - val_acc: 0.3649\n",
      "\n",
      "Epoch 00001: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.001.h5\n",
      "Epoch 2/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 1.4834 - acc: 0.5861 - val_loss: 1.5869 - val_acc: 0.5457\n",
      "\n",
      "Epoch 00002: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.002.h5\n",
      "Epoch 3/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 1.2703 - acc: 0.6598 - val_loss: 1.6084 - val_acc: 0.5704\n",
      "\n",
      "Epoch 00003: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.003.h5\n",
      "Epoch 4/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 1.1366 - acc: 0.7060 - val_loss: 2.0095 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00004: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.004.h5\n",
      "Epoch 5/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 1.0422 - acc: 0.7395 - val_loss: 1.3844 - val_acc: 0.6388\n",
      "\n",
      "Epoch 00005: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.005.h5\n",
      "Epoch 6/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.9724 - acc: 0.7640 - val_loss: 1.4376 - val_acc: 0.6196\n",
      "\n",
      "Epoch 00006: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.006.h5\n",
      "Epoch 7/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.8995 - acc: 0.7904 - val_loss: 1.7276 - val_acc: 0.5810\n",
      "\n",
      "Epoch 00007: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.007.h5\n",
      "Epoch 8/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.8458 - acc: 0.8124 - val_loss: 1.5326 - val_acc: 0.6315\n",
      "\n",
      "Epoch 00008: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.008.h5\n",
      "Epoch 9/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.8076 - acc: 0.8271 - val_loss: 1.9792 - val_acc: 0.5523\n",
      "\n",
      "Epoch 00009: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.009.h5\n",
      "Epoch 10/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 52ms/step - loss: 0.7680 - acc: 0.8429 - val_loss: 1.8295 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00010: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.010.h5\n",
      "Epoch 11/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.7219 - acc: 0.8608 - val_loss: 1.8054 - val_acc: 0.5982\n",
      "\n",
      "Epoch 00011: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.011.h5\n",
      "Epoch 12/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.7063 - acc: 0.8678 - val_loss: 2.2149 - val_acc: 0.5592\n",
      "\n",
      "Epoch 00012: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.012.h5\n",
      "Epoch 13/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.6820 - acc: 0.8808 - val_loss: 1.6978 - val_acc: 0.6444\n",
      "\n",
      "Epoch 00013: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.013.h5\n",
      "Epoch 14/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.6547 - acc: 0.8895 - val_loss: 2.2391 - val_acc: 0.5910\n",
      "\n",
      "Epoch 00014: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.014.h5\n",
      "Epoch 15/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.6497 - acc: 0.8954 - val_loss: 1.4915 - val_acc: 0.6839\n",
      "\n",
      "Epoch 00015: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.015.h5\n",
      "Epoch 16/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.6188 - acc: 0.9094 - val_loss: 2.4542 - val_acc: 0.5576\n",
      "\n",
      "Epoch 00016: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.016.h5\n",
      "Epoch 17/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.6190 - acc: 0.9101 - val_loss: 2.2065 - val_acc: 0.5671\n",
      "\n",
      "Epoch 00017: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.017.h5\n",
      "Epoch 18/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.6239 - acc: 0.9102 - val_loss: 1.8552 - val_acc: 0.6518\n",
      "\n",
      "Epoch 00018: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.018.h5\n",
      "Epoch 19/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5996 - acc: 0.9193 - val_loss: 1.8178 - val_acc: 0.6536\n",
      "\n",
      "Epoch 00019: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.019.h5\n",
      "Epoch 20/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5878 - acc: 0.9250 - val_loss: 2.0735 - val_acc: 0.6336\n",
      "\n",
      "Epoch 00021: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.021.h5\n",
      "Epoch 22/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5839 - acc: 0.9285 - val_loss: 2.6981 - val_acc: 0.5683\n",
      "\n",
      "Epoch 00022: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.022.h5\n",
      "Epoch 23/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5724 - acc: 0.9339 - val_loss: 2.4026 - val_acc: 0.6051\n",
      "\n",
      "Epoch 00023: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.023.h5\n",
      "Epoch 24/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5947 - acc: 0.9270 - val_loss: 2.0533 - val_acc: 0.6291\n",
      "\n",
      "Epoch 00024: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.024.h5\n",
      "Epoch 25/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5884 - acc: 0.9284 - val_loss: 2.3671 - val_acc: 0.5935\n",
      "\n",
      "Epoch 00025: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.025.h5\n",
      "Epoch 26/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5681 - acc: 0.9371 - val_loss: 1.8413 - val_acc: 0.6697\n",
      "\n",
      "Epoch 00026: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.026.h5\n",
      "Epoch 27/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5720 - acc: 0.9359 - val_loss: 1.6073 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00027: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.027.h5\n",
      "Epoch 28/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5773 - acc: 0.9347 - val_loss: 1.9313 - val_acc: 0.6833\n",
      "\n",
      "Epoch 00028: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.028.h5\n",
      "Epoch 29/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5770 - acc: 0.9347 - val_loss: 1.9364 - val_acc: 0.6893\n",
      "\n",
      "Epoch 00029: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.029.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5694 - acc: 0.9376 - val_loss: 1.8014 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00030: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.030.h5\n",
      "Epoch 31/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5741 - acc: 0.9371 - val_loss: 1.8443 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00031: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.031.h5\n",
      "Epoch 32/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5626 - acc: 0.9409 - val_loss: 2.0267 - val_acc: 0.6606\n",
      "\n",
      "Epoch 00032: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.032.h5\n",
      "Epoch 33/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 48ms/step - loss: 0.5656 - acc: 0.9428 - val_loss: 1.8690 - val_acc: 0.6623\n",
      "\n",
      "Epoch 00033: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.033.h5\n",
      "Epoch 34/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5709 - acc: 0.9392 - val_loss: 2.0953 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00034: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.034.h5\n",
      "Epoch 35/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5626 - acc: 0.9432 - val_loss: 2.3915 - val_acc: 0.6271\n",
      "\n",
      "Epoch 00035: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.035.h5\n",
      "Epoch 36/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5541 - acc: 0.9462 - val_loss: 2.4486 - val_acc: 0.6456\n",
      "\n",
      "Epoch 00036: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.036.h5\n",
      "Epoch 37/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5650 - acc: 0.9417 - val_loss: 2.2716 - val_acc: 0.6273\n",
      "\n",
      "Epoch 00037: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.037.h5\n",
      "Epoch 38/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 48ms/step - loss: 0.5727 - acc: 0.9396 - val_loss: 1.8027 - val_acc: 0.6922\n",
      "\n",
      "Epoch 00038: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.038.h5\n",
      "Epoch 39/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5508 - acc: 0.9469 - val_loss: 2.0765 - val_acc: 0.6577\n",
      "\n",
      "Epoch 00039: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.039.h5\n",
      "Epoch 40/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5365 - acc: 0.9522 - val_loss: 2.0324 - val_acc: 0.6812\n",
      "\n",
      "Epoch 00040: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.040.h5\n",
      "Epoch 41/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5630 - acc: 0.9432 - val_loss: 2.1020 - val_acc: 0.6474\n",
      "\n",
      "Epoch 00041: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.041.h5\n",
      "Epoch 42/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 48ms/step - loss: 0.5523 - acc: 0.9453 - val_loss: 2.4232 - val_acc: 0.6216\n",
      "\n",
      "Epoch 00042: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.042.h5\n",
      "Epoch 43/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 37s 53ms/step - loss: 0.5483 - acc: 0.9488 - val_loss: 2.0058 - val_acc: 0.6404\n",
      "\n",
      "Epoch 00043: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.043.h5\n",
      "Epoch 44/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 52ms/step - loss: 0.5506 - acc: 0.9460 - val_loss: 2.1114 - val_acc: 0.6547\n",
      "\n",
      "Epoch 00044: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.044.h5\n",
      "Epoch 45/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5351 - acc: 0.9523 - val_loss: 1.9637 - val_acc: 0.6664\n",
      "\n",
      "Epoch 00045: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.045.h5\n",
      "Epoch 46/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5671 - acc: 0.9421 - val_loss: 1.6616 - val_acc: 0.7128\n",
      "\n",
      "Epoch 00046: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.046.h5\n",
      "Epoch 47/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5603 - acc: 0.9441 - val_loss: 1.8909 - val_acc: 0.6786\n",
      "\n",
      "Epoch 00047: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.047.h5\n",
      "Epoch 48/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 48ms/step - loss: 0.5505 - acc: 0.9464 - val_loss: 2.6695 - val_acc: 0.5634\n",
      "\n",
      "Epoch 00048: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.048.h5\n",
      "Epoch 49/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 48ms/step - loss: 0.5453 - acc: 0.9487 - val_loss: 1.9931 - val_acc: 0.6718\n",
      "\n",
      "Epoch 00049: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.049.h5\n",
      "Epoch 50/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5323 - acc: 0.9519 - val_loss: 1.8995 - val_acc: 0.6843\n",
      "\n",
      "Epoch 00050: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.050.h5\n",
      "Epoch 51/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 48ms/step - loss: 0.5709 - acc: 0.9406 - val_loss: 1.8387 - val_acc: 0.6782\n",
      "\n",
      "Epoch 00051: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.051.h5\n",
      "Epoch 52/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5244 - acc: 0.9569 - val_loss: 1.7259 - val_acc: 0.7023\n",
      "\n",
      "Epoch 00052: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.052.h5\n",
      "Epoch 53/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5545 - acc: 0.9441 - val_loss: 1.8061 - val_acc: 0.6841\n",
      "\n",
      "Epoch 00053: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.053.h5\n",
      "Epoch 54/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5301 - acc: 0.9530 - val_loss: 1.9965 - val_acc: 0.6674\n",
      "\n",
      "Epoch 00054: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.054.h5\n",
      "Epoch 55/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5584 - acc: 0.9442 - val_loss: 1.8080 - val_acc: 0.6853\n",
      "\n",
      "Epoch 00055: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.055.h5\n",
      "Epoch 56/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5187 - acc: 0.9571 - val_loss: 1.9222 - val_acc: 0.6992\n",
      "\n",
      "Epoch 00056: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.056.h5\n",
      "Epoch 57/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5209 - acc: 0.9556 - val_loss: 1.7155 - val_acc: 0.7086\n",
      "\n",
      "Epoch 00057: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.057.h5\n",
      "Epoch 58/200\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5403 - acc: 0.9481 - val_loss: 1.9410 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00058: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.058.h5\n",
      "Epoch 59/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5232 - acc: 0.9549 - val_loss: 1.7876 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00059: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.059.h5\n",
      "Epoch 60/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5403 - acc: 0.9481 - val_loss: 2.4364 - val_acc: 0.6350\n",
      "\n",
      "Epoch 00060: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.060.h5\n",
      "Epoch 61/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.5242 - acc: 0.9543 - val_loss: 2.0781 - val_acc: 0.6653\n",
      "\n",
      "Epoch 00061: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.061.h5\n",
      "Epoch 62/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5275 - acc: 0.9524 - val_loss: 2.0208 - val_acc: 0.6646\n",
      "\n",
      "Epoch 00062: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.062.h5\n",
      "Epoch 63/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5176 - acc: 0.9557 - val_loss: 1.6786 - val_acc: 0.7076\n",
      "\n",
      "Epoch 00063: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.063.h5\n",
      "Epoch 64/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5219 - acc: 0.9539 - val_loss: 1.9071 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00064: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.064.h5\n",
      "Epoch 65/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5198 - acc: 0.9556 - val_loss: 1.7366 - val_acc: 0.7079\n",
      "\n",
      "Epoch 00065: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.065.h5\n",
      "Epoch 66/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5348 - acc: 0.9494 - val_loss: 1.8430 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00066: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.066.h5\n",
      "Epoch 67/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5062 - acc: 0.9596 - val_loss: 2.7854 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00067: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.067.h5\n",
      "Epoch 68/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5301 - acc: 0.9494 - val_loss: 2.1559 - val_acc: 0.6650\n",
      "\n",
      "Epoch 00068: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.068.h5\n",
      "Epoch 69/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5242 - acc: 0.9533 - val_loss: 1.7859 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00069: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.069.h5\n",
      "Epoch 70/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5281 - acc: 0.9508 - val_loss: 2.0426 - val_acc: 0.6761\n",
      "\n",
      "Epoch 00070: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.070.h5\n",
      "Epoch 71/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5182 - acc: 0.9542 - val_loss: 1.9776 - val_acc: 0.6633\n",
      "\n",
      "Epoch 00071: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.071.h5\n",
      "Epoch 72/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 50ms/step - loss: 0.5168 - acc: 0.9551 - val_loss: 2.1025 - val_acc: 0.6537\n",
      "\n",
      "Epoch 00072: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.072.h5\n",
      "Epoch 73/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5345 - acc: 0.9483 - val_loss: 1.9082 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00073: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.073.h5\n",
      "Epoch 74/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.5219 - acc: 0.9526 - val_loss: 1.9689 - val_acc: 0.6781\n",
      "\n",
      "Epoch 00074: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.074.h5\n",
      "Epoch 75/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 34s 48ms/step - loss: 0.5248 - acc: 0.9516 - val_loss: 1.8794 - val_acc: 0.6776\n",
      "\n",
      "Epoch 00075: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.075.h5\n",
      "Epoch 76/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.5215 - acc: 0.9524 - val_loss: 2.0583 - val_acc: 0.6730\n",
      "\n",
      "Epoch 00076: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.076.h5\n",
      "Epoch 77/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 39s 55ms/step - loss: 0.5155 - acc: 0.9551 - val_loss: 2.3640 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00077: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.077.h5\n",
      "Epoch 78/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.4983 - acc: 0.9602 - val_loss: 2.3505 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00078: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.078.h5\n",
      "Epoch 79/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5154 - acc: 0.9534 - val_loss: 1.7496 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00079: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.079.h5\n",
      "Epoch 80/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5216 - acc: 0.9526 - val_loss: 1.7983 - val_acc: 0.6897\n",
      "\n",
      "Epoch 00080: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.080.h5\n",
      "Epoch 81/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.5072 - acc: 0.9569 - val_loss: 1.8783 - val_acc: 0.7026\n",
      "\n",
      "Epoch 00081: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.081.h5\n",
      "Epoch 82/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.4554 - acc: 0.9763 - val_loss: 1.4228 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00082: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.082.h5\n",
      "Epoch 83/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.3972 - acc: 0.9974 - val_loss: 1.4530 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00083: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.083.h5\n",
      "Epoch 84/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.3818 - acc: 0.9997 - val_loss: 1.4773 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00084: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.084.h5\n",
      "Epoch 85/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.3704 - acc: 0.9998 - val_loss: 1.5119 - val_acc: 0.7677\n",
      "\n",
      "Epoch 00085: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.085.h5\n",
      "Epoch 86/200\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 35s 49ms/step - loss: 0.3586 - acc: 0.9997 - val_loss: 1.5283 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00086: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.086.h5\n",
      "Epoch 87/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.3433 - acc: 0.9999 - val_loss: 1.5712 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00087: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.087.h5\n",
      "Epoch 88/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.3271 - acc: 0.9999 - val_loss: 1.5620 - val_acc: 0.7687\n",
      "\n",
      "Epoch 00088: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.088.h5\n",
      "Epoch 89/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.3102 - acc: 0.9997 - val_loss: 1.5812 - val_acc: 0.7672\n",
      "\n",
      "Epoch 00089: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.089.h5\n",
      "Epoch 90/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.2950 - acc: 0.9995 - val_loss: 1.6234 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00090: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.090.h5\n",
      "Epoch 91/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.2794 - acc: 0.9999 - val_loss: 1.6188 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00091: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.091.h5\n",
      "Epoch 92/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.2677 - acc: 0.9992 - val_loss: 1.6362 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00092: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.092.h5\n",
      "Epoch 93/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.2563 - acc: 0.9994 - val_loss: 1.6154 - val_acc: 0.7679\n",
      "\n",
      "Epoch 00093: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.093.h5\n",
      "Epoch 94/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.2435 - acc: 0.9999 - val_loss: 1.6392 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00094: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.094.h5\n",
      "Epoch 95/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.2351 - acc: 0.9990 - val_loss: 1.6788 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00095: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.095.h5\n",
      "Epoch 96/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.2238 - acc: 0.9998 - val_loss: 1.6630 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00096: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.096.h5\n",
      "Epoch 97/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.2185 - acc: 0.9988 - val_loss: 1.6852 - val_acc: 0.7663\n",
      "\n",
      "Epoch 00097: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.097.h5\n",
      "Epoch 98/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.2099 - acc: 0.9995 - val_loss: 1.7168 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00098: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.098.h5\n",
      "Epoch 99/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.2091 - acc: 0.9980 - val_loss: 1.6724 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00099: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.099.h5\n",
      "Epoch 100/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.1992 - acc: 0.9997 - val_loss: 1.7259 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00100: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.100.h5\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1932 - acc: 0.9998 - val_loss: 1.7008 - val_acc: 0.7657\n",
      "\n",
      "Epoch 00101: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.101.h5\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1870 - acc: 0.9998 - val_loss: 1.7640 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00102: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.102.h5\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.1822 - acc: 0.9994 - val_loss: 1.9039 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00103: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.103.h5\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.1834 - acc: 0.9980 - val_loss: 1.7208 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00104: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.104.h5\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.1750 - acc: 0.9991 - val_loss: 1.7897 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00105: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.105.h5\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.1698 - acc: 0.9996 - val_loss: 1.7464 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00106: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.106.h5\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.1650 - acc: 0.9997 - val_loss: 1.9611 - val_acc: 0.7441\n",
      "\n",
      "Epoch 00107: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.107.h5\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 34s 49ms/step - loss: 0.1684 - acc: 0.9973 - val_loss: 1.8009 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00108: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.108.h5\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.1628 - acc: 0.9982 - val_loss: 1.7400 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00109: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.109.h5\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.1590 - acc: 0.9988 - val_loss: 1.8148 - val_acc: 0.7561\n",
      "\n",
      "Epoch 00110: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.110.h5\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1548 - acc: 0.9993 - val_loss: 1.7433 - val_acc: 0.7627\n",
      "\n",
      "Epoch 00111: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.111.h5\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1572 - acc: 0.9977 - val_loss: 1.7649 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00112: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.112.h5\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1532 - acc: 0.9984 - val_loss: 1.7439 - val_acc: 0.7654\n",
      "\n",
      "Epoch 00113: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.113.h5\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1470 - acc: 0.9996 - val_loss: 1.8119 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00114: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.114.h5\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1503 - acc: 0.9981 - val_loss: 1.8571 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00115: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.115.h5\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.1431 - acc: 0.9995 - val_loss: 1.8611 - val_acc: 0.7547\n",
      "\n",
      "Epoch 00116: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.116.h5\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1409 - acc: 0.9995 - val_loss: 1.9127 - val_acc: 0.7504\n",
      "\n",
      "Epoch 00117: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.117.h5\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.1387 - acc: 0.9993 - val_loss: 1.8481 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00118: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.118.h5\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1384 - acc: 0.9987 - val_loss: 1.9754 - val_acc: 0.7424\n",
      "\n",
      "Epoch 00119: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.119.h5\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 36s 51ms/step - loss: 0.1407 - acc: 0.9974 - val_loss: 1.8186 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00120: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.120.h5\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0001\n",
      "704/704 [==============================] - 35s 49ms/step - loss: 0.1330 - acc: 0.9994 - val_loss: 1.8749 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00121: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.121.h5\n",
      "Epoch 122/200\n",
      "Learning rate:  1e-05\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1311 - acc: 0.9995 - val_loss: 1.7667 - val_acc: 0.7679\n",
      "\n",
      "Epoch 00122: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.122.h5\n",
      "Epoch 123/200\n",
      "Learning rate:  1e-05\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1298 - acc: 0.9999 - val_loss: 1.7624 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00123: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.123.h5\n",
      "Epoch 124/200\n",
      "Learning rate:  1e-05\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1294 - acc: 0.9999 - val_loss: 1.7583 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00124: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.124.h5\n",
      "Epoch 125/200\n",
      "Learning rate:  1e-05\n",
      "704/704 [==============================] - 35s 50ms/step - loss: 0.1289 - acc: 1.0000 - val_loss: 1.7583 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00125: saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model_no_mean.125.h5\n",
      "Epoch 126/200\n",
      "Learning rate:  1e-05\n",
      "679/704 [===========================>..] - ETA: 1s - loss: 0.1286 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "# enable this if pydot can be installed\n",
    "# pip install pydot\n",
    "#plot_model(model, to_file=\"%s.png\" % model_type, show_shapes=True)\n",
    "print(model_type)\n",
    "\n",
    "# prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'save_models')\n",
    "model_name = 'av11_%s_model_no_mean.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=False)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # this will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    steps_per_epoch =  math.ceil(len(x_train) / batch_size)\n",
    "    # fit the model on the batches generated by datagen.flow().\n",
    "    model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "              verbose=1,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e62bcca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           activation_6[0][0]               \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 16)   0           activation_10[0][0]              \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 16)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2320        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 16)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2320        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 16)   0           activation_12[0][0]              \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 16)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 32)   4640        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 32)   9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   544         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 32)   0           conv2d_17[0][0]                  \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 32)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 32)   128         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 32)   128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 32)   0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 32)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   9248        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 32)   0           activation_18[0][0]              \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 32)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9248        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 32)   128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9248        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 32)   0           activation_20[0][0]              \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 32)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 32)   128         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 32)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 32)   0           activation_22[0][0]              \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 32)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 32)   9248        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 32)   128         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 32)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9248        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 32)   0           activation_24[0][0]              \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 32)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 32)   9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 32)   128         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 32)   9248        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 32)   0           activation_26[0][0]              \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 32)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 64)     18496       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 64)     256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 64)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 64)     2112        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 64)     0           conv2d_32[0][0]                  \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 64)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 64)     36928       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 64)     256         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 64)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 64)     36928       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 64)     256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 64)     0           activation_30[0][0]              \n",
      "                                                                 batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 64)     0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 64)     36928       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 64)     256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 64)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 64)     36928       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 64)     256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 64)     0           activation_32[0][0]              \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 64)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 64)     36928       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 64)     36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 64)     0           activation_34[0][0]              \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 64)     0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 64)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 64)     36928       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 8, 8, 64)     0           activation_36[0][0]              \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 64)     0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 64)     256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36928       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 64)     0           activation_38[0][0]              \n",
      "                                                                 batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 64)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 64)     36928       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 64)     256         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 64)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     36928       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 8, 8, 64)     0           activation_40[0][0]              \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 64)     0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 9)            585         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 665,929\n",
      "Trainable params: 662,761\n",
      "Non-trainable params: 3,168\n",
      "__________________________________________________________________________________________________\n",
      "ResNet44v1\n",
      "Not using data augmentation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 49s 50ms/step - loss: 2.1877 - acc: 0.3543 - val_loss: 1.8185 - val_acc: 0.4618\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.46178, saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model.001.h5\n",
      "Epoch 2/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 31s 44ms/step - loss: 1.5064 - acc: 0.5807 - val_loss: 1.6276 - val_acc: 0.5500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.46178 to 0.55000, saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model.002.h5\n",
      "Epoch 3/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 31s 45ms/step - loss: 1.2956 - acc: 0.6545 - val_loss: 1.6838 - val_acc: 0.5357\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55000\n",
      "Epoch 4/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 33s 46ms/step - loss: 1.1551 - acc: 0.7021 - val_loss: 1.6083 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.55000 to 0.56978, saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model.004.h5\n",
      "Epoch 5/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 32s 46ms/step - loss: 1.0510 - acc: 0.7389 - val_loss: 1.9524 - val_acc: 0.5025\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.56978\n",
      "Epoch 6/200\n",
      "Learning rate:  0.001\n",
      "704/704 [==============================] - 33s 47ms/step - loss: 0.9803 - acc: 0.7631 - val_loss: 1.6648 - val_acc: 0.6070\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.56978 to 0.60704, saving model to /home/jrkim/jsp/AdversarialExample/other_explain_model/save_models/av11_ResNet44v1_model.006.h5\n",
      "Epoch 7/200\n",
      "Learning rate:  0.001\n",
      "505/704 [====================>.........] - ETA: 8s - loss: 0.8983 - acc: 0.7931"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fb84981badad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m               callbacks=callbacks)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using real-time data augmentation.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/jsp/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# score trained model\n",
    "scores = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9ff8f",
   "metadata": {},
   "source": [
    "### 오류해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ef2ab",
   "metadata": {},
   "source": [
    ">cifar10 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc248029",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train2, y_train2), (x_test2, y_test2) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3b2ac4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c8886753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape :  (32, 32, 3)\n",
      "x_train2 shape: (50000, 32, 32, 3)\n",
      "50000 train2 samples\n",
      "10000 test2 samples\n",
      "y_train2 shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions.\n",
    "input_shape = x_train2.shape[1:]\n",
    "print('input_shape : ', input_shape)\n",
    "# normalize data.\n",
    "x_train2 = x_train2.astype('float32') / 255\n",
    "x_test2 = x_test2.astype('float32') / 255\n",
    "\n",
    "# if subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train2_mean = np.mean(x_train2, axis=0)\n",
    "    x_train2 -= x_train2_mean\n",
    "    x_test2 -= x_train2_mean\n",
    "\n",
    "print('x_train2 shape:', x_train2.shape)\n",
    "print(x_train2.shape[0], 'train2 samples')\n",
    "print(x_test2.shape[0], 'test2 samples')\n",
    "print('y_train2 shape:', y_train2.shape)\n",
    "\n",
    "# convert class vectors to binary class matrices.\n",
    "y_train2 = to_categorical(y_train2, num_classes)\n",
    "y_test2 = to_categorical(y_test2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bd4713d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31495, 11)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c33e65a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape :  (32, 32, 3)\n",
      "x_train shape: (133641, 32, 32, 3)\n",
      "133641 train samples\n",
      "31495 test samples\n",
      "y_train shape: (133641, 1)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "print('input_shape : ', input_shape)\n",
    "# normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# if subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# convert class vectors to binary class matrices.\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb589e",
   "metadata": {},
   "source": [
    "## 수정 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5006630f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5ef98d34c8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f0e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    train_dir='../BDD100K_MOT2020_image/bdd100k/images/track/final_train_av'\n",
    "    test_dir='../BDD100K_MOT2020_image/bdd100k/images/track/final_test_av'\n",
    "\n",
    "    batch_size = 128\n",
    "    img_height = 32\n",
    "    img_width = 32\n",
    "\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      train_dir,\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      test_dir,\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7627785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    train_dir='../BDD100K_MOT2020_image/bdd100k/images/track/final_train_av'\n",
    "    test_dir='../BDD100K_MOT2020_image/bdd100k/images/track/final_test_av'\n",
    "\n",
    "    batch_size = 128\n",
    "    img_height = 32\n",
    "    img_width = 32\n",
    "\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      train_dir,\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      test_dir,\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a51b9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 10 classes.\n",
      "Found 3000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds=get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e025c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'bicycle', 'bus', 'car', 'motorcycle', 'other person', 'other vehicle', 'pedestrian', 'rider', 'truck']\n"
     ]
    }
   ],
   "source": [
    "class_names = test_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a53a208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9890ce",
   "metadata": {},
   "source": [
    "### Batchdataset to numpy  \n",
    "133641\n",
    "31495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44525720",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-290b597b9f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab9b2a54",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BatchDataset in module tensorflow.python.data.ops.dataset_ops object:\n",
      "\n",
      "class BatchDataset(UnaryDataset)\n",
      " |  A `Dataset` that batches contiguous elements from its input.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchDataset\n",
      " |      UnaryDataset\n",
      " |      DatasetV2\n",
      " |      collections.abc.Iterable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_dataset, batch_size, drop_remainder)\n",
      " |      See `Dataset.batch()` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  element_spec\n",
      " |      The type specification of an element of this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset.element_spec\n",
      " |      TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
      " |      \n",
      " |      For more information,\n",
      " |      read [this guide](https://www.tensorflow.org/guide/data#dataset_structure).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A (nested) structure of `tf.TypeSpec` objects matching the structure of an\n",
      " |        element of this dataset and specifying the type of individual components.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DatasetV2:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Creates an iterator for elements of this dataset.\n",
      " |      \n",
      " |      The returned iterator implements the Python Iterator protocol.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `tf.data.Iterator` for the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If not inside of tf.function and not executing eagerly.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the length of the dataset if it is known and finite.\n",
      " |      \n",
      " |      This method requires that you are running in eager mode, and that the\n",
      " |      length of the dataset is known and non-infinite. When the length may be\n",
      " |      unknown or infinite, or if you are running in graph mode, use\n",
      " |      `tf.data.Dataset.cardinality` instead.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An integer representing the length of the dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the dataset length is unknown or infinite, or if eager\n",
      " |          execution is not enabled.\n",
      " |  \n",
      " |  __nonzero__ = __bool__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply(self, transformation_func)\n",
      " |      Applies a transformation function to this dataset.\n",
      " |      \n",
      " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
      " |      represented as functions that take one `Dataset` argument and return a\n",
      " |      transformed `Dataset`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(100)\n",
      " |      >>> def dataset_fn(ds):\n",
      " |      ...   return ds.filter(lambda x: x < 5)\n",
      " |      >>> dataset = dataset.apply(dataset_fn)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Args:\n",
      " |        transformation_func: A function that takes one `Dataset` argument and\n",
      " |          returns a `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` returned by applying `transformation_func` to this\n",
      " |            dataset.\n",
      " |  \n",
      " |  as_numpy_iterator(self)\n",
      " |      Returns an iterator which converts all elements of the dataset to numpy.\n",
      " |      \n",
      " |      Use `as_numpy_iterator` to inspect the content of your dataset. To see\n",
      " |      element shapes and types, print dataset elements directly instead of using\n",
      " |      `as_numpy_iterator`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> for element in dataset:\n",
      " |      ...   print(element)\n",
      " |      tf.Tensor(1, shape=(), dtype=int32)\n",
      " |      tf.Tensor(2, shape=(), dtype=int32)\n",
      " |      tf.Tensor(3, shape=(), dtype=int32)\n",
      " |      \n",
      " |      This method requires that you are running in eager mode and the dataset's\n",
      " |      element_spec contains only `TensorSpec` components.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      1\n",
      " |      2\n",
      " |      3\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> print(list(dataset.as_numpy_iterator()))\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      `as_numpy_iterator()` will preserve the nested structure of dataset\n",
      " |      elements.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
      " |      ...                                               'b': [5, 6]})\n",
      " |      >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
      " |      ...                                       {'a': (2, 4), 'b': 6}]\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        An iterable over the elements of the dataset, with their tensors converted\n",
      " |        to numpy arrays.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if an element contains a non-`Tensor` value.\n",
      " |        RuntimeError: if eager execution is not enabled.\n",
      " |  \n",
      " |  batch(self, batch_size, drop_remainder=False, num_parallel_calls=None, deterministic=None)\n",
      " |      Combines consecutive elements of this dataset into batches.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(8)\n",
      " |      >>> dataset = dataset.batch(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(8)\n",
      " |      >>> dataset = dataset.batch(3, drop_remainder=True)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([0, 1, 2]), array([3, 4, 5])]\n",
      " |      \n",
      " |      The components of the resulting element will have an additional outer\n",
      " |      dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
      " |      element if `batch_size` does not divide the number of input elements `N`\n",
      " |      evenly and `drop_remainder` is `False`). If your program depends on the\n",
      " |      batches having the same outer dimension, you should set the `drop_remainder`\n",
      " |      argument to `True` to prevent the smaller batch from being produced.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n",
      " |          representing the number of batches to compute asynchronously in\n",
      " |          parallel.\n",
      " |          If not specified, batches will be computed sequentially. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available resources.\n",
      " |        deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
      " |          boolean is specified (`True` or `False`), it controls the order in which\n",
      " |          the transformation produces elements. If set to `False`, the\n",
      " |          transformation is allowed to yield elements out of order to trade\n",
      " |          determinism for performance. If not specified, the\n",
      " |          `tf.data.Options.experimental_deterministic` option\n",
      " |          (`True` by default) controls the behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cache(self, filename='')\n",
      " |      Caches the elements in this dataset.\n",
      " |      \n",
      " |      The first time the dataset is iterated over, its elements will be cached\n",
      " |      either in the specified file or in memory. Subsequent iterations will\n",
      " |      use the cached data.\n",
      " |      \n",
      " |      Note: For the cache to be finalized, the input dataset must be iterated\n",
      " |      through in its entirety. Otherwise, subsequent iterations will not use\n",
      " |      cached data.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(5)\n",
      " |      >>> dataset = dataset.map(lambda x: x**2)\n",
      " |      >>> dataset = dataset.cache()\n",
      " |      >>> # The first time reading through the data will generate the data using\n",
      " |      >>> # `range` and `map`.\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 4, 9, 16]\n",
      " |      >>> # Subsequent iterations read from the cache.\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 4, 9, 16]\n",
      " |      \n",
      " |      When caching to a file, the cached data will persist across runs. Even the\n",
      " |      first iteration through the data will read from the cache file. Changing\n",
      " |      the input pipeline before the call to `.cache()` will have no effect until\n",
      " |      the cache file is removed or the filename is changed.\n",
      " |      \n",
      " |      ```python\n",
      " |      dataset = tf.data.Dataset.range(5)\n",
      " |      dataset = dataset.cache(\"/path/to/file\")\n",
      " |      list(dataset.as_numpy_iterator())\n",
      " |      # [0, 1, 2, 3, 4]\n",
      " |      dataset = tf.data.Dataset.range(10)\n",
      " |      dataset = dataset.cache(\"/path/to/file\")  # Same file!\n",
      " |      list(dataset.as_numpy_iterator())\n",
      " |      # [0, 1, 2, 3, 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Note: `cache` will produce exactly the same elements during each iteration\n",
      " |      through the dataset. If you wish to randomize the iteration order, make sure\n",
      " |      to call `shuffle` *after* calling `cache`.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
      " |          directory on the filesystem to use for caching elements in this Dataset.\n",
      " |          If a filename is not provided, the dataset will be cached in memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cardinality(self)\n",
      " |      Returns the cardinality of the dataset, if known.\n",
      " |      \n",
      " |      `cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\n",
      " |      contains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\n",
      " |      the analysis fails to determine the number of elements in the dataset\n",
      " |      (e.g. when the dataset source is a file).\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(42)\n",
      " |      >>> print(dataset.cardinality().numpy())\n",
      " |      42\n",
      " |      >>> dataset = dataset.repeat()\n",
      " |      >>> cardinality = dataset.cardinality()\n",
      " |      >>> print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\n",
      " |      True\n",
      " |      >>> dataset = dataset.filter(lambda x: True)\n",
      " |      >>> cardinality = dataset.cardinality()\n",
      " |      >>> print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A scalar `tf.int64` `Tensor` representing the cardinality of the dataset.\n",
      " |        If the cardinality is infinite or unknown, `cardinality` returns the\n",
      " |        named constants `tf.data.INFINITE_CARDINALITY` and\n",
      " |        `tf.data.UNKNOWN_CARDINALITY` respectively.\n",
      " |  \n",
      " |  concatenate(self, dataset)\n",
      " |      Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      " |      \n",
      " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      >>> b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
      " |      >>> ds = a.concatenate(b)\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [1, 2, 3, 4, 5, 6, 7]\n",
      " |      >>> # The input dataset and dataset to be concatenated should have\n",
      " |      >>> # compatible element specs.\n",
      " |      >>> c = tf.data.Dataset.zip((a, b))\n",
      " |      >>> a.concatenate(c)\n",
      " |      Traceback (most recent call last):\n",
      " |      TypeError: Two datasets to concatenate have different types\n",
      " |      <dtype: 'int64'> and (tf.int64, tf.int64)\n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
      " |      >>> a.concatenate(d)\n",
      " |      Traceback (most recent call last):\n",
      " |      TypeError: Two datasets to concatenate have different types\n",
      " |      <dtype: 'int64'> and <dtype: 'string'>\n",
      " |      \n",
      " |      Args:\n",
      " |        dataset: `Dataset` to be concatenated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  enumerate(self, start=0)\n",
      " |      Enumerates the elements of this dataset.\n",
      " |      \n",
      " |      It is similar to python's `enumerate`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.enumerate(start=5)\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (5, 1)\n",
      " |      (6, 2)\n",
      " |      (7, 3)\n",
      " |      \n",
      " |      >>> # The (nested) structure of the input dataset determines the\n",
      " |      >>> # structure of elements in the resulting dataset.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
      " |      >>> dataset = dataset.enumerate()\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (0, array([7, 8], dtype=int32))\n",
      " |      (1, array([ 9, 10], dtype=int32))\n",
      " |      \n",
      " |      Args:\n",
      " |        start: A `tf.int64` scalar `tf.Tensor`, representing the start value for\n",
      " |          enumeration.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  filter(self, predicate)\n",
      " |      Filters this dataset according to `predicate`.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.filter(lambda x: x < 3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2]\n",
      " |      >>> # `tf.math.equal(x, y)` is required for equality comparison\n",
      " |      >>> def filter_fn(x):\n",
      " |      ...   return tf.math.equal(x, 1)\n",
      " |      >>> dataset = dataset.filter(filter_fn)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1]\n",
      " |      \n",
      " |      Args:\n",
      " |        predicate: A function mapping a dataset element to a boolean.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` containing the elements of this dataset for which\n",
      " |            `predicate` is `True`.\n",
      " |  \n",
      " |  flat_map(self, map_func)\n",
      " |      Maps `map_func` across this dataset and flattens the result.\n",
      " |      \n",
      " |      Use `flat_map` if you want to make sure that the order of your dataset\n",
      " |      stays the same. For example, to flatten a dataset of batches into a\n",
      " |      dataset of their elements:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(\n",
      " |      ...                [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
      " |      >>> dataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |      \n",
      " |      `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n",
      " |      `flat_map` produces the same output as\n",
      " |      `tf.data.Dataset.interleave(cycle_length=1)`\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  interleave(self, map_func, cycle_length=None, block_length=None, num_parallel_calls=None, deterministic=None)\n",
      " |      Maps `map_func` across this dataset, and interleaves the results.\n",
      " |      \n",
      " |      For example, you can use `Dataset.interleave()` to process many input files\n",
      " |      concurrently:\n",
      " |      \n",
      " |      >>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
      " |      >>> # from each file.\n",
      " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
      " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
      " |      >>> def parse_fn(filename):\n",
      " |      ...   return tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.interleave(lambda x:\n",
      " |      ...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
      " |      ...     cycle_length=4, block_length=16)\n",
      " |      \n",
      " |      The `cycle_length` and `block_length` arguments control the order in which\n",
      " |      elements are produced. `cycle_length` controls the number of input elements\n",
      " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
      " |      transformation will handle one input element at a time, and will produce\n",
      " |      identical results to `tf.data.Dataset.flat_map`. In general,\n",
      " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
      " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
      " |      producing `block_length` consecutive elements from each iterator, and\n",
      " |      consuming the next input element each time it reaches the end of an\n",
      " |      iterator.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> # NOTE: New lines indicate \"block\" boundaries.\n",
      " |      >>> dataset = dataset.interleave(\n",
      " |      ...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
      " |      ...     cycle_length=2, block_length=4)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 1, 1, 1,\n",
      " |       2, 2, 2, 2,\n",
      " |       1, 1,\n",
      " |       2, 2,\n",
      " |       3, 3, 3, 3,\n",
      " |       4, 4, 4, 4,\n",
      " |       3, 3,\n",
      " |       4, 4,\n",
      " |       5, 5, 5, 5,\n",
      " |       5, 5]\n",
      " |      \n",
      " |      Note: The order of elements yielded by this transformation is\n",
      " |      deterministic, as long as `map_func` is a pure function and\n",
      " |      `deterministic=True`. If `map_func` contains any stateful operations, the\n",
      " |      order in which that state is accessed is undefined.\n",
      " |      \n",
      " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
      " |      `interleave` will use multiple threads to fetch elements. If determinism\n",
      " |      isn't required, it can also improve performance to set\n",
      " |      `deterministic=False`.\n",
      " |      \n",
      " |      >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
      " |      ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
      " |      >>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
      " |      ...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
      " |      ...     deterministic=False)\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |        cycle_length: (Optional.) The number of input elements that will be\n",
      " |          processed concurrently. If not set, the tf.data runtime decides what it\n",
      " |          should be based on available CPU. If `num_parallel_calls` is set to\n",
      " |          `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
      " |          the maximum degree of parallelism.\n",
      " |        block_length: (Optional.) The number of consecutive elements to produce\n",
      " |          from each input element before cycling to another input element. If not\n",
      " |          set, defaults to 1.\n",
      " |        num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
      " |          threadpool, which is used to fetch inputs from cycle elements\n",
      " |          asynchronously and in parallel. The default behavior is to fetch inputs\n",
      " |          from cycle elements synchronously with no parallelism. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |        deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
      " |          boolean is specified (`True` or `False`), it controls the order in which\n",
      " |          the transformation produces elements. If set to `False`, the\n",
      " |          transformation is allowed to yield elements out of order to trade\n",
      " |          determinism for performance. If not specified, the\n",
      " |          `tf.data.Options.experimental_deterministic` option\n",
      " |          (`True` by default) controls the behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  map(self, map_func, num_parallel_calls=None, deterministic=None)\n",
      " |      Maps `map_func` across the elements of this dataset.\n",
      " |      \n",
      " |      This transformation applies `map_func` to each element of this dataset, and\n",
      " |      returns a new dataset containing the transformed elements, in the same\n",
      " |      order as they appeared in the input. `map_func` can be used to change both\n",
      " |      the values and the structure of a dataset's elements. Supported structure\n",
      " |      constructs are documented\n",
      " |      [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
      " |      \n",
      " |      For example, `map` can be used for adding 1 to each element, or projecting a\n",
      " |      subset of element components.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> dataset = dataset.map(lambda x: x + 1)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      \n",
      " |      The input signature of `map_func` is determined by the structure of each\n",
      " |      element in this dataset.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(5)\n",
      " |      >>> # `map_func` takes a single argument of type `tf.Tensor` with the same\n",
      " |      >>> # shape and dtype.\n",
      " |      >>> result = dataset.map(lambda x: x + 1)\n",
      " |      \n",
      " |      >>> # Each element is a tuple containing two `tf.Tensor` objects.\n",
      " |      >>> elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: elements, (tf.int32, tf.string))\n",
      " |      >>> # `map_func` takes two arguments of type `tf.Tensor`. This function\n",
      " |      >>> # projects out just the first component.\n",
      " |      >>> result = dataset.map(lambda x_int, y_str: x_int)\n",
      " |      >>> list(result.as_numpy_iterator())\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      >>> # Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
      " |      >>> elements =  ([{\"a\": 1, \"b\": \"foo\"},\n",
      " |      ...               {\"a\": 2, \"b\": \"bar\"},\n",
      " |      ...               {\"a\": 3, \"b\": \"baz\"}])\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n",
      " |      >>> # `map_func` takes a single argument of type `dict` with the same keys\n",
      " |      >>> # as the elements.\n",
      " |      >>> result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n",
      " |      \n",
      " |      The value or values returned by `map_func` determine the structure of each\n",
      " |      element in the returned dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> # `map_func` returns two `tf.Tensor` objects.\n",
      " |      >>> def g(x):\n",
      " |      ...   return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
      " |      >>> result = dataset.map(g)\n",
      " |      >>> result.element_spec\n",
      " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n",
      " |      >>> # Python primitives, lists, and NumPy arrays are implicitly converted to\n",
      " |      >>> # `tf.Tensor`.\n",
      " |      >>> def h(x):\n",
      " |      ...   return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
      " |      >>> result = dataset.map(h)\n",
      " |      >>> result.element_spec\n",
      " |      (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n",
      " |      >>> # `map_func` can return nested structures.\n",
      " |      >>> def i(x):\n",
      " |      ...   return (37.0, [42, 16]), \"foo\"\n",
      " |      >>> result = dataset.map(i)\n",
      " |      >>> result.element_spec\n",
      " |      ((TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
      " |        TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n",
      " |       TensorSpec(shape=(), dtype=tf.string, name=None))\n",
      " |      \n",
      " |      `map_func` can accept as arguments and return any type of dataset element.\n",
      " |      \n",
      " |      Note that irrespective of the context in which `map_func` is defined (eager\n",
      " |      vs. graph), tf.data traces the function and executes it as a graph. To use\n",
      " |      Python code inside of the function you have a few options:\n",
      " |      \n",
      " |      1) Rely on AutoGraph to convert Python code into an equivalent graph\n",
      " |      computation. The downside of this approach is that AutoGraph can convert\n",
      " |      some but not all Python code.\n",
      " |      \n",
      " |      2) Use `tf.py_function`, which allows you to write arbitrary Python code but\n",
      " |      will generally result in worse performance than 1). For example:\n",
      " |      \n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
      " |      >>> # transform a string tensor to upper case string using a Python function\n",
      " |      >>> def upper_case_fn(t: tf.Tensor):\n",
      " |      ...   return t.numpy().decode('utf-8').upper()\n",
      " |      >>> d = d.map(lambda x: tf.py_function(func=upper_case_fn,\n",
      " |      ...           inp=[x], Tout=tf.string))\n",
      " |      >>> list(d.as_numpy_iterator())\n",
      " |      [b'HELLO', b'WORLD']\n",
      " |      \n",
      " |      3) Use `tf.numpy_function`, which also allows you to write arbitrary\n",
      " |      Python code. Note that `tf.py_function` accepts `tf.Tensor` whereas\n",
      " |      `tf.numpy_function` accepts numpy arrays and returns only numpy arrays.\n",
      " |      For example:\n",
      " |      \n",
      " |      >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
      " |      >>> def upper_case_fn(t: np.ndarray):\n",
      " |      ...   return t.decode('utf-8').upper()\n",
      " |      >>> d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n",
      " |      ...           inp=[x], Tout=tf.string))\n",
      " |      >>> list(d.as_numpy_iterator())\n",
      " |      [b'HELLO', b'WORLD']\n",
      " |      \n",
      " |      Note that the use of `tf.numpy_function` and `tf.py_function`\n",
      " |      in general precludes the possibility of executing user-defined\n",
      " |      transformations in parallel (because of Python GIL).\n",
      " |      \n",
      " |      Performance can often be improved by setting `num_parallel_calls` so that\n",
      " |      `map` will use multiple threads to process elements. If deterministic order\n",
      " |      isn't required, it can also improve performance to set\n",
      " |      `deterministic=False`.\n",
      " |      \n",
      " |      >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      >>> dataset = dataset.map(lambda x: x + 1,\n",
      " |      ...     num_parallel_calls=tf.data.AUTOTUNE,\n",
      " |      ...     deterministic=False)\n",
      " |      \n",
      " |      The order of elements yielded by this transformation is deterministic if\n",
      " |      `deterministic=True`. If `map_func` contains stateful operations and\n",
      " |      `num_parallel_calls > 1`, the order in which that state is accessed is\n",
      " |      undefined, so the values of output elements may not be deterministic\n",
      " |      regardless of the `deterministic` flag value.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to another dataset element.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n",
      " |          representing the number elements to process asynchronously in parallel.\n",
      " |          If not specified, elements will be processed sequentially. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |        deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
      " |          boolean is specified (`True` or `False`), it controls the order in which\n",
      " |          the transformation produces elements. If set to `False`, the\n",
      " |          transformation is allowed to yield elements out of order to trade\n",
      " |          determinism for performance. If not specified, the\n",
      " |          `tf.data.Options.experimental_deterministic` option\n",
      " |          (`True` by default) controls the behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  options(self)\n",
      " |      Returns the options for this dataset and its inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.data.Options` object representing the dataset options.\n",
      " |  \n",
      " |  padded_batch(self, batch_size, padded_shapes=None, padding_values=None, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into padded batches.\n",
      " |      \n",
      " |      This transformation combines multiple consecutive elements of the input\n",
      " |      dataset into a single element.\n",
      " |      \n",
      " |      Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
      " |      have an additional outer dimension, which will be `batch_size` (or\n",
      " |      `N % batch_size` for the last element if `batch_size` does not divide the\n",
      " |      number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
      " |      your program depends on the batches having the same outer dimension, you\n",
      " |      should set the `drop_remainder` argument to `True` to prevent the smaller\n",
      " |      batch from being produced.\n",
      " |      \n",
      " |      Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
      " |      different shapes, and this transformation will pad each component to the\n",
      " |      respective shape in `padded_shapes`. The `padded_shapes` argument\n",
      " |      determines the resulting shape for each dimension of each component in an\n",
      " |      output element:\n",
      " |      \n",
      " |      * If the dimension is a constant, the component will be padded out to that\n",
      " |        length in that dimension.\n",
      " |      * If the dimension is unknown, the component will be padded out to the\n",
      " |        maximum length of all elements in that dimension.\n",
      " |      \n",
      " |      >>> A = (tf.data.Dataset\n",
      " |      ...      .range(1, 5, output_type=tf.int32)\n",
      " |      ...      .map(lambda x: tf.fill([x], x)))\n",
      " |      >>> # Pad to the smallest per-batch size that fits all elements.\n",
      " |      >>> B = A.padded_batch(2)\n",
      " |      >>> for element in B.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[1 0]\n",
      " |       [2 2]]\n",
      " |      [[3 3 3 0]\n",
      " |       [4 4 4 4]]\n",
      " |      >>> # Pad to a fixed size.\n",
      " |      >>> C = A.padded_batch(2, padded_shapes=5)\n",
      " |      >>> for element in C.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[1 0 0 0 0]\n",
      " |       [2 2 0 0 0]]\n",
      " |      [[3 3 3 0 0]\n",
      " |       [4 4 4 4 0]]\n",
      " |      >>> # Pad with a custom value.\n",
      " |      >>> D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
      " |      >>> for element in D.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      [[ 1 -1 -1 -1 -1]\n",
      " |       [ 2  2 -1 -1 -1]]\n",
      " |      [[ 3  3  3 -1 -1]\n",
      " |       [ 4  4  4  4 -1]]\n",
      " |      >>> # Components of nested elements can be padded independently.\n",
      " |      >>> elements = [([1, 2, 3], [10]),\n",
      " |      ...             ([4, 5], [11, 12])]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...     lambda: iter(elements), (tf.int32, tf.int32))\n",
      " |      >>> # Pad the first component of the tuple to length 4, and the second\n",
      " |      >>> # component to the smallest size that fits.\n",
      " |      >>> dataset = dataset.padded_batch(2,\n",
      " |      ...     padded_shapes=([4], [None]),\n",
      " |      ...     padding_values=(-1, 100))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n",
      " |        array([[ 10, 100], [ 11,  12]], dtype=int32))]\n",
      " |      >>> # Pad with a single value and multiple components.\n",
      " |      >>> E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
      " |      >>> for element in E.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (array([[ 1, -1],\n",
      " |             [ 2,  2]], dtype=int32), array([[ 1, -1],\n",
      " |             [ 2,  2]], dtype=int32))\n",
      " |      (array([[ 3,  3,  3, -1],\n",
      " |             [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n",
      " |             [ 4,  4,  4,  4]], dtype=int32))\n",
      " |      \n",
      " |      See also `tf.data.experimental.dense_to_sparse_batch`, which combines\n",
      " |      elements that may have different shapes into a `tf.sparse.SparseTensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        padded_shapes: (Optional.) A (nested) structure of `tf.TensorShape` or\n",
      " |          `tf.int64` vector tensor-like objects representing the shape to which\n",
      " |          the respective component of each input element should be padded prior\n",
      " |          to batching. Any unknown dimensions will be padded to the maximum size\n",
      " |          of that dimension in each batch. If unset, all dimensions of all\n",
      " |          components are padded to the maximum size in the batch. `padded_shapes`\n",
      " |          must be set if any component has an unknown rank.\n",
      " |        padding_values: (Optional.) A (nested) structure of scalar-shaped\n",
      " |          `tf.Tensor`, representing the padding values to use for the respective\n",
      " |          components. None represents that the (nested) structure should be padded\n",
      " |          with default values.  Defaults are `0` for numeric types and the empty\n",
      " |          string for string types. The `padding_values` should have the same\n",
      " |          (nested) structure as the input dataset. If `padding_values` is a single\n",
      " |          element and the input dataset has multiple components, then the same\n",
      " |          `padding_values` will be used to pad every component of the dataset.\n",
      " |          If `padding_values` is a scalar, then its value will be broadcasted\n",
      " |          to match the shape of each component.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a component has an unknown rank, and  the `padded_shapes`\n",
      " |          argument is not set.\n",
      " |  \n",
      " |  prefetch(self, buffer_size)\n",
      " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
      " |      \n",
      " |      Most dataset input pipelines should end with a call to `prefetch`. This\n",
      " |      allows later elements to be prepared while the current element is being\n",
      " |      processed. This often improves latency and throughput, at the cost of\n",
      " |      using additional memory to store prefetched elements.\n",
      " |      \n",
      " |      Note: Like other `Dataset` methods, prefetch operates on the\n",
      " |      elements of the input dataset. It has no concept of examples vs. batches.\n",
      " |      `examples.prefetch(2)` will prefetch two elements (2 examples),\n",
      " |      while `examples.batch(20).prefetch(2)` will prefetch 2 elements\n",
      " |      (2 batches, of 20 examples each).\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(3)\n",
      " |      >>> dataset = dataset.prefetch(2)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum\n",
      " |          number of elements that will be buffered when prefetching. If the value\n",
      " |          `tf.data.AUTOTUNE` is used, then the buffer size is dynamically tuned.\n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  reduce(self, initial_state, reduce_func)\n",
      " |      Reduces the input dataset to a single element.\n",
      " |      \n",
      " |      The transformation calls `reduce_func` successively on every element of\n",
      " |      the input dataset until the dataset is exhausted, aggregating information in\n",
      " |      its internal state. The `initial_state` argument is used for the initial\n",
      " |      state and the final state is returned as the result.\n",
      " |      \n",
      " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n",
      " |      5\n",
      " |      >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n",
      " |      10\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_state: An element representing the initial state of the\n",
      " |          transformation.\n",
      " |        reduce_func: A function that maps `(old_state, input_element)` to\n",
      " |          `new_state`. It must take two arguments and return a new element\n",
      " |          The structure of `new_state` must match the structure of\n",
      " |          `initial_state`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A dataset element corresponding to the final state of the transformation.\n",
      " |  \n",
      " |  repeat(self, count=None)\n",
      " |      Repeats this dataset so each original value is seen `count` times.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> dataset = dataset.repeat(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
      " |      \n",
      " |      Note: If this dataset is a function of global state (e.g. a random number\n",
      " |      generator), then different repetitions may produce different elements.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of times the dataset should be repeated. The default behavior (if\n",
      " |          `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  shard(self, num_shards, index)\n",
      " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      " |      \n",
      " |      `shard` is deterministic. The Dataset produced by `A.shard(n, i)` will\n",
      " |      contain all elements of A whose index mod n = i.\n",
      " |      \n",
      " |      >>> A = tf.data.Dataset.range(10)\n",
      " |      >>> B = A.shard(num_shards=3, index=0)\n",
      " |      >>> list(B.as_numpy_iterator())\n",
      " |      [0, 3, 6, 9]\n",
      " |      >>> C = A.shard(num_shards=3, index=1)\n",
      " |      >>> list(C.as_numpy_iterator())\n",
      " |      [1, 4, 7]\n",
      " |      >>> D = A.shard(num_shards=3, index=2)\n",
      " |      >>> list(D.as_numpy_iterator())\n",
      " |      [2, 5, 8]\n",
      " |      \n",
      " |      This dataset operator is very useful when running distributed training, as\n",
      " |      it allows each worker to read a unique subset.\n",
      " |      \n",
      " |      When reading a single input file, you can shard elements as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.TFRecordDataset(input_file)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Important caveats:\n",
      " |      \n",
      " |      - Be sure to shard before you use any randomizing operator (such as\n",
      " |        shuffle).\n",
      " |      - Generally it is best if the shard operator is used early in the dataset\n",
      " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
      " |        before converting the dataset to input samples. This avoids reading every\n",
      " |        file on every worker. The following is an example of an efficient\n",
      " |        sharding strategy within a complete pipeline:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = Dataset.list_files(pattern)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.interleave(tf.data.TFRecordDataset,\n",
      " |                       cycle_length=num_readers, block_length=1)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          shards operating in parallel.\n",
      " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        InvalidArgumentError: if `num_shards` or `index` are illegal values.\n",
      " |      \n",
      " |          Note: error checking is done on a best-effort basis, and errors aren't\n",
      " |          guaranteed to be caught upon dataset creation. (e.g. providing in a\n",
      " |          placeholder tensor bypasses the early checking, and will instead result\n",
      " |          in an error during a session.run call.)\n",
      " |  \n",
      " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
      " |      Randomly shuffles the elements of this dataset.\n",
      " |      \n",
      " |      This dataset fills a buffer with `buffer_size` elements, then randomly\n",
      " |      samples elements from this buffer, replacing the selected elements with new\n",
      " |      elements. For perfect shuffling, a buffer size greater than or equal to the\n",
      " |      full size of the dataset is required.\n",
      " |      \n",
      " |      For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
      " |      set to 1,000, then `shuffle` will initially select a random element from\n",
      " |      only the first 1,000 elements in the buffer. Once an element is selected,\n",
      " |      its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
      " |      maintaining the 1,000 element buffer.\n",
      " |      \n",
      " |      `reshuffle_each_iteration` controls whether the shuffle order should be\n",
      " |      different for each epoch. In TF 1.X, the idiomatic way to create epochs\n",
      " |      was through the `repeat` transformation:\n",
      " |      \n",
      " |      ```python\n",
      " |      dataset = tf.data.Dataset.range(3)\n",
      " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      " |      dataset = dataset.repeat(2)\n",
      " |      # [1, 0, 2, 1, 2, 0]\n",
      " |      \n",
      " |      dataset = tf.data.Dataset.range(3)\n",
      " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      " |      dataset = dataset.repeat(2)\n",
      " |      # [1, 0, 2, 1, 0, 2]\n",
      " |      ```\n",
      " |      \n",
      " |      In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\n",
      " |      possible to also create epochs through Python iteration:\n",
      " |      \n",
      " |      ```python\n",
      " |      dataset = tf.data.Dataset.range(3)\n",
      " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
      " |      list(dataset.as_numpy_iterator())\n",
      " |      # [1, 0, 2]\n",
      " |      list(dataset.as_numpy_iterator())\n",
      " |      # [1, 2, 0]\n",
      " |      ```\n",
      " |      \n",
      " |      ```python\n",
      " |      dataset = tf.data.Dataset.range(3)\n",
      " |      dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
      " |      list(dataset.as_numpy_iterator())\n",
      " |      # [1, 0, 2]\n",
      " |      list(dataset.as_numpy_iterator())\n",
      " |      # [1, 0, 2]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements from this dataset from which the new dataset will sample.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.random.set_seed` for behavior.\n",
      " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
      " |          iterated over. (Defaults to `True`.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  skip(self, count)\n",
      " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.skip(7)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [7, 8, 9]\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be skipped to form the new dataset.\n",
      " |          If `count` is greater than the size of this dataset, the new dataset\n",
      " |          will contain no elements.  If `count` is -1, skips the entire dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  take(self, count)\n",
      " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(10)\n",
      " |      >>> dataset = dataset.take(3)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [0, 1, 2]\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be taken to form the new dataset.\n",
      " |          If `count` is -1, or if `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain all elements of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  unbatch(self)\n",
      " |      Splits elements of a dataset into multiple elements.\n",
      " |      \n",
      " |      For example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\n",
      " |      where `B` may vary for each input element, then for each element in the\n",
      " |      dataset, the unbatched dataset will contain `B` consecutive elements\n",
      " |      of shape `[a0, a1, ...]`.\n",
      " |      \n",
      " |      >>> elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\n",
      " |      >>> dataset = dataset.unbatch()\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3, 1, 2, 1, 2, 3, 4]\n",
      " |      \n",
      " |      Note: `unbatch` requires a data copy to slice up the batched tensor into\n",
      " |      smaller, unbatched tensors. When optimizing performance, try to avoid\n",
      " |      unnecessary usage of `unbatch`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset`.\n",
      " |  \n",
      " |  window(self, size, shift=None, stride=1, drop_remainder=False)\n",
      " |      Combines (nests of) input elements into a dataset of (nests of) windows.\n",
      " |      \n",
      " |      A \"window\" is a finite dataset of flat elements of size `size` (or possibly\n",
      " |      fewer if there are not enough input elements to fill the window and\n",
      " |      `drop_remainder` evaluates to `False`).\n",
      " |      \n",
      " |      The `shift` argument determines the number of input elements by which the\n",
      " |      window moves on each iteration.  If windows and elements are both numbered\n",
      " |      starting at 0, the first element in window `k` will be element `k * shift`\n",
      " |      of the input dataset. In particular, the first element of the first window\n",
      " |      will always be the first element of the input dataset.\n",
      " |      \n",
      " |      The `stride` argument determines the stride of the input elements, and the\n",
      " |      `shift` argument determines the shift of the window.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 1]\n",
      " |      [2, 3]\n",
      " |      [4, 5]\n",
      " |      [6]\n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 1, 2]\n",
      " |      [2, 3, 4]\n",
      " |      [4, 5, 6]\n",
      " |      >>> dataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   print(list(window.as_numpy_iterator()))\n",
      " |      [0, 2, 4]\n",
      " |      [1, 3, 5]\n",
      " |      [2, 4, 6]\n",
      " |      \n",
      " |      Note that when the `window` transformation is applied to a dataset of\n",
      " |      nested elements, it produces a dataset of nested windows.\n",
      " |      \n",
      " |      >>> nested = ([1, 2, 3, 4], [5, 6, 7, 8])\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   def to_numpy(ds):\n",
      " |      ...     return list(ds.as_numpy_iterator())\n",
      " |      ...   print(tuple(to_numpy(component) for component in window))\n",
      " |      ([1, 2], [5, 6])\n",
      " |      ([3, 4], [7, 8])\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\n",
      " |      >>> dataset = dataset.window(2)\n",
      " |      >>> for window in dataset:\n",
      " |      ...   def to_numpy(ds):\n",
      " |      ...     return list(ds.as_numpy_iterator())\n",
      " |      ...   print({'a': to_numpy(window['a'])})\n",
      " |      {'a': [1, 2]}\n",
      " |      {'a': [3, 4]}\n",
      " |      \n",
      " |      Args:\n",
      " |        size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\n",
      " |          of the input dataset to combine into a window. Must be positive.\n",
      " |        shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of input elements by which the window moves in each iteration.\n",
      " |          Defaults to `size`. Must be positive.\n",
      " |        stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          stride of the input elements in the sliding window. Must be positive.\n",
      " |          The default value of 1 means \"retain every input element\".\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last windows should be dropped if their size is smaller than\n",
      " |          `size`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` of (nests of) windows -- a finite datasets of flat\n",
      " |          elements created from the (nests of) input elements.\n",
      " |  \n",
      " |  with_options(self, options)\n",
      " |      Returns a new `tf.data.Dataset` with the given options set.\n",
      " |      \n",
      " |      The options are \"global\" in the sense they apply to the entire dataset.\n",
      " |      If options are set multiple times, they are merged as long as different\n",
      " |      options do not use different non-default values.\n",
      " |      \n",
      " |      >>> ds = tf.data.Dataset.range(5)\n",
      " |      >>> ds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n",
      " |      ...                    cycle_length=3,\n",
      " |      ...                    num_parallel_calls=3)\n",
      " |      >>> options = tf.data.Options()\n",
      " |      >>> # This will make the interleave order non-deterministic.\n",
      " |      >>> options.experimental_deterministic = False\n",
      " |      >>> ds = ds.with_options(options)\n",
      " |      \n",
      " |      Args:\n",
      " |        options: A `tf.data.Options` that identifies the options the use.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` with the given options.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: when an option is set more than once to a non-default value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from DatasetV2:\n",
      " |  \n",
      " |  from_generator(generator, output_types=None, output_shapes=None, args=None, output_signature=None)\n",
      " |      Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(output_shapes, output_types)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use output_signature instead\n",
      " |      \n",
      " |      The `generator` argument must be a callable object that returns\n",
      " |      an object that supports the `iter()` protocol (e.g. a generator function).\n",
      " |      \n",
      " |      The elements generated by `generator` must be compatible with either the\n",
      " |      given `output_signature` argument or with the given `output_types` and\n",
      " |      (optionally) `output_shapes` arguments, whichever was specified.\n",
      " |      \n",
      " |      The recommended way to call `from_generator` is to use the\n",
      " |      `output_signature` argument. In this case the output will be assumed to\n",
      " |      consist of objects with the classes, shapes and types defined by\n",
      " |      `tf.TypeSpec` objects from `output_signature` argument:\n",
      " |      \n",
      " |      >>> def gen():\n",
      " |      ...   ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n",
      " |      ...   yield 42, ragged_tensor\n",
      " |      >>>\n",
      " |      >>> dataset = tf.data.Dataset.from_generator(\n",
      " |      ...      gen,\n",
      " |      ...      output_signature=(\n",
      " |      ...          tf.TensorSpec(shape=(), dtype=tf.int32),\n",
      " |      ...          tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n",
      " |      >>>\n",
      " |      >>> list(dataset.take(1))\n",
      " |      [(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n",
      " |      <tf.RaggedTensor [[1, 2], [3]]>)]\n",
      " |      \n",
      " |      There is also a deprecated way to call `from_generator` by either with\n",
      " |      `output_types` argument alone or together with `output_shapes` argument.\n",
      " |      In this case the output of the function will be assumed to consist of\n",
      " |      `tf.Tensor` objects with the types defined by `output_types` and with the\n",
      " |      shapes which are either unknown or defined by `output_shapes`.\n",
      " |      \n",
      " |      Note: The current implementation of `Dataset.from_generator()` uses\n",
      " |      `tf.numpy_function` and inherits the same constraints. In particular, it\n",
      " |      requires the dataset and iterator related operations to be placed\n",
      " |      on a device in the same process as the Python program that called\n",
      " |      `Dataset.from_generator()`. The body of `generator` will not be\n",
      " |      serialized in a `GraphDef`, and you should not use this method if you\n",
      " |      need to serialize your model and restore it in a different environment.\n",
      " |      \n",
      " |      Note: If `generator` depends on mutable global variables or other external\n",
      " |      state, be aware that the runtime may invoke `generator` multiple times\n",
      " |      (in order to support repeating the `Dataset`) and at any time\n",
      " |      between the call to `Dataset.from_generator()` and the production of the\n",
      " |      first element from the generator. Mutating global variables or external\n",
      " |      state can cause undefined behavior, and we recommend that you explicitly\n",
      " |      cache any external state in `generator` before calling\n",
      " |      `Dataset.from_generator()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        generator: A callable object that returns an object that supports the\n",
      " |          `iter()` protocol. If `args` is not specified, `generator` must take no\n",
      " |          arguments; otherwise it must take as many arguments as there are values\n",
      " |          in `args`.\n",
      " |        output_types: (Optional.) A (nested) structure of `tf.DType` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |        output_shapes: (Optional.) A (nested) structure of `tf.TensorShape`\n",
      " |          objects corresponding to each component of an element yielded by\n",
      " |          `generator`.\n",
      " |        args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
      " |          and passed to `generator` as NumPy-array arguments.\n",
      " |        output_signature: (Optional.) A (nested) structure of `tf.TypeSpec`\n",
      " |          objects corresponding to each component of an element yielded by\n",
      " |          `generator`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensor_slices(tensors)\n",
      " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
      " |      \n",
      " |      The given tensors are sliced along their first dimension. This operation\n",
      " |      preserves the structure of the input tensors, removing the first dimension\n",
      " |      of each tensor and using it as the dataset dimension. All input tensors\n",
      " |      must have the same size in their first dimensions.\n",
      " |      \n",
      " |      >>> # Slicing a 1D tensor produces scalar tensor elements.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [1, 2, 3]\n",
      " |      \n",
      " |      >>> # Slicing a 2D tensor produces 1D tensor elements.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
      " |      \n",
      " |      >>> # Slicing a tuple of 1D tensors produces tuple elements containing\n",
      " |      >>> # scalar tensors.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(1, 3, 5), (2, 4, 6)]\n",
      " |      \n",
      " |      >>> # Dictionary structure is also preserved.\n",
      " |      >>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\n",
      " |      >>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n",
      " |      ...                                       {'a': 2, 'b': 4}]\n",
      " |      True\n",
      " |      \n",
      " |      >>> # Two tensors can be combined into one Dataset object.\n",
      " |      >>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
      " |      >>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
      " |      >>> dataset = Dataset.from_tensor_slices((features, labels))\n",
      " |      >>> # Both the features and the labels tensors can be converted\n",
      " |      >>> # to a Dataset object separately and combined after.\n",
      " |      >>> features_dataset = Dataset.from_tensor_slices(features)\n",
      " |      >>> labels_dataset = Dataset.from_tensor_slices(labels)\n",
      " |      >>> dataset = Dataset.zip((features_dataset, labels_dataset))\n",
      " |      >>> # A batched feature and label set can be converted to a Dataset\n",
      " |      >>> # in similar fashion.\n",
      " |      >>> batched_features = tf.constant([[[1, 3], [2, 3]],\n",
      " |      ...                                 [[2, 1], [1, 2]],\n",
      " |      ...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
      " |      >>> batched_labels = tf.constant([['A', 'A'],\n",
      " |      ...                               ['B', 'B'],\n",
      " |      ...                               ['A', 'B']], shape=(3, 2, 1))\n",
      " |      >>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
      " |      >>> for element in dataset.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (array([[1, 3],\n",
      " |             [2, 3]], dtype=int32), array([[b'A'],\n",
      " |             [b'A']], dtype=object))\n",
      " |      (array([[2, 1],\n",
      " |             [1, 2]], dtype=int32), array([[b'B'],\n",
      " |             [b'B']], dtype=object))\n",
      " |      (array([[3, 3],\n",
      " |             [3, 2]], dtype=int32), array([[b'A'],\n",
      " |             [b'B']], dtype=object))\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this guide](\n",
      " |      https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset element, whose components have the same first\n",
      " |          dimension. Supported values are documented\n",
      " |          [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensors(tensors)\n",
      " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
      " |      \n",
      " |      `from_tensors` produces a dataset containing only a single element. To slice\n",
      " |      the input tensor into multiple elements, use `from_tensor_slices` instead.\n",
      " |      \n",
      " |      >>> dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2, 3], dtype=int32)]\n",
      " |      >>> dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [(array([1, 2, 3], dtype=int32), b'A')]\n",
      " |      \n",
      " |      >>> # You can use `from_tensors` to produce a dataset which repeats\n",
      " |      >>> # the same example many times.\n",
      " |      >>> example = tf.constant([1,2,3])\n",
      " |      >>> dataset = tf.data.Dataset.from_tensors(example).repeat(2)\n",
      " |      >>> list(dataset.as_numpy_iterator())\n",
      " |      [array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this\n",
      " |      guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset \"element\". Supported values are documented\n",
      " |          [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  list_files(file_pattern, shuffle=None, seed=None)\n",
      " |      A dataset of all files matching one or more glob patterns.\n",
      " |      \n",
      " |      The `file_pattern` argument should be a small number of glob patterns.\n",
      " |      If your filenames have already been globbed, use\n",
      " |      `Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\n",
      " |      filename with `list_files` may result in poor performance with remote\n",
      " |      storage systems.\n",
      " |      \n",
      " |      Note: The default behavior of this method is to return filenames in\n",
      " |      a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
      " |      to get results in a deterministic order.\n",
      " |      \n",
      " |      Example:\n",
      " |        If we had the following files on our filesystem:\n",
      " |      \n",
      " |          - /path/to/dir/a.txt\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset\n",
      " |        would produce:\n",
      " |      \n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |      Args:\n",
      " |        file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
      " |          (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
      " |          pattern(s) that will be matched.\n",
      " |        shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
      " |          Defaults to `True`.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.random.set_seed` for behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |       Dataset: A `Dataset` of strings corresponding to file names.\n",
      " |  \n",
      " |  range(*args, **kwargs)\n",
      " |      Creates a `Dataset` of a step-separated range of values.\n",
      " |      \n",
      " |      >>> list(Dataset.range(5).as_numpy_iterator())\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> list(Dataset.range(2, 5).as_numpy_iterator())\n",
      " |      [2, 3, 4]\n",
      " |      >>> list(Dataset.range(1, 5, 2).as_numpy_iterator())\n",
      " |      [1, 3]\n",
      " |      >>> list(Dataset.range(1, 5, -2).as_numpy_iterator())\n",
      " |      []\n",
      " |      >>> list(Dataset.range(5, 1).as_numpy_iterator())\n",
      " |      []\n",
      " |      >>> list(Dataset.range(5, 1, -2).as_numpy_iterator())\n",
      " |      [5, 3]\n",
      " |      >>> list(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n",
      " |      [2, 3, 4]\n",
      " |      >>> list(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n",
      " |      [1.0, 3.0]\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: follows the same semantics as python's xrange.\n",
      " |          len(args) == 1 -> start = 0, stop = args[0], step = 1.\n",
      " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1.\n",
      " |          len(args) == 3 -> start = args[0], stop = args[1], step = args[2].\n",
      " |        **kwargs:\n",
      " |          - output_type: Its expected dtype. (Optional, default: `tf.int64`).\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `RangeDataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if len(args) == 0.\n",
      " |  \n",
      " |  zip(datasets)\n",
      " |      Creates a `Dataset` by zipping together the given datasets.\n",
      " |      \n",
      " |      This method has similar semantics to the built-in `zip()` function\n",
      " |      in Python, with the main difference being that the `datasets`\n",
      " |      argument can be a (nested) structure of `Dataset` objects. The supported\n",
      " |      nesting mechanisms are documented\n",
      " |      [here] (https://www.tensorflow.org/guide/data#dataset_structure).\n",
      " |      \n",
      " |      >>> # The nested structure of the `datasets` argument determines the\n",
      " |      >>> # structure of elements in the resulting dataset.\n",
      " |      >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      >>> b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, b))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(1, 4), (2, 5), (3, 6)]\n",
      " |      >>> ds = tf.data.Dataset.zip((b, a))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(4, 1), (5, 2), (6, 3)]\n",
      " |      >>>\n",
      " |      >>> # The `datasets` argument may contain an arbitrary number of datasets.\n",
      " |      >>> c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
      " |      ...                                            #       [9, 10],\n",
      " |      ...                                            #       [11, 12] ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, b, c))\n",
      " |      >>> for element in ds.as_numpy_iterator():\n",
      " |      ...   print(element)\n",
      " |      (1, 4, array([7, 8]))\n",
      " |      (2, 5, array([ 9, 10]))\n",
      " |      (3, 6, array([11, 12]))\n",
      " |      >>>\n",
      " |      >>> # The number of elements in the resulting dataset is the same as\n",
      " |      >>> # the size of the smallest dataset in `datasets`.\n",
      " |      >>> d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
      " |      >>> ds = tf.data.Dataset.zip((a, d))\n",
      " |      >>> list(ds.as_numpy_iterator())\n",
      " |      [(1, 13), (2, 14)]\n",
      " |      \n",
      " |      Args:\n",
      " |        datasets: A (nested) structure of datasets.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DatasetV2:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Iterable:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700ead83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_numpy(train_ds, test_ds):\n",
    "\n",
    "    # av dataset\n",
    "    # 0610 : 9개의 클래스에 대해 train(!0000), test(3000)\n",
    "    x_train_base=np.zeros([90000, 32, 32, 3])\n",
    "    y_train_base=np.zeros([90000, 1])\n",
    "    x_test_base=np.zeros([27000, 32, 32, 3])\n",
    "    y_test_base=np.zeros([27000, 1])\n",
    "\n",
    "    numpy_iter=list(train_ds.as_numpy_iterator())\n",
    "\n",
    "    for idx, batch in enumerate(numpy_iter):\n",
    "    #     if idx%30==0:\n",
    "    #         print(idx)\n",
    "        x_train_base[128*idx:128*(idx+1), :, :, :]=batch[0]\n",
    "        y_train_base[128*idx:128*(idx+1), 0]=batch[1]\n",
    "\n",
    "    numpy_iter_test=list(test_ds.as_numpy_iterator())\n",
    "\n",
    "    len(numpy_iter_test)\n",
    "\n",
    "    for idx, batch in enumerate(numpy_iter_test):\n",
    "    #     if idx%30==0:\n",
    "    #         print(idx)\n",
    "        x_test_base[128*idx:128*(idx+1), :, :, :]=batch[0]\n",
    "        y_test_base[128*idx:128*(idx+1), 0]=batch[1]\n",
    "    \n",
    "    return x_train_base, y_train_base, x_test_base, y_test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920fea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
